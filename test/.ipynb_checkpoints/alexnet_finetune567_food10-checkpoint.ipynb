{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is disabled, cuDNN 5105)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "(2000,)\n",
      "Data loaded !\n",
      "Test train Shape: \n",
      "(1000, 3, 227, 227)\n",
      "(1000, 3, 227, 227)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/machine/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode='max')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Fold 1 / 2\n",
      "Fine-tuning CNN..\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 6s - loss: 2.7187 - acc: 0.1010 - val_loss: 2.2561 - val_acc: 0.1420\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 6s - loss: 2.5224 - acc: 0.1010 - val_loss: 2.1654 - val_acc: 0.2840\n",
      "Softmax acc: 28.40%\n",
      "Generating train features for SVM..\n",
      "(1000, 4096)\n",
      "Generating test features for SVM..\n",
      "(1000, 4096)\n",
      "\n",
      "Training SVM..\n",
      "SVM acc: : 61.50%\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "           AisKacang       0.60      0.71      0.65       100\n",
      "           AngKuKueh       0.55      0.63      0.59       100\n",
      "           ApamBalik       0.68      0.63      0.65       100\n",
      "           Asamlaksa       0.79      0.61      0.69       100\n",
      "              Bahulu       0.55      0.58      0.57       100\n",
      "           Bakkukteh       0.50      0.48      0.49       100\n",
      "      BananaLeafRice       0.60      0.57      0.58       100\n",
      "             Bazhang       0.61      0.62      0.61       100\n",
      "         BeefRendang       0.47      0.44      0.45       100\n",
      "           BingkaUbi       0.85      0.88      0.86       100\n",
      "\n",
      "         avg / total       0.62      0.61      0.61      1000\n",
      "\n",
      "[[71  0  6  0  0  4  2  4 13  0]\n",
      " [ 1 63  0  0 19  5  6  1  2  3]\n",
      " [ 6  6 63  0  1  3  7  1 11  2]\n",
      " [ 5  6  2 61  9  9  3  1  2  2]\n",
      " [ 1 14  4  3 58  5  0 11  3  1]\n",
      " [ 1  3  3  9  6 48  7  8 13  2]\n",
      " [ 9  6  8  1  1  9 57  4  4  1]\n",
      " [ 4 12  3  1  6  5  2 62  2  3]\n",
      " [17  4  2  2  5  8 10  6 44  2]\n",
      " [ 4  1  2  0  0  0  1  4  0 88]]\n",
      "Test train Shape: \n",
      "(1000, 3, 227, 227)\n",
      "(1000, 3, 227, 227)\n",
      "Running Fold 2 / 2\n",
      "Fine-tuning CNN..\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 6s - loss: 2.6931 - acc: 0.0890 - val_loss: 2.2746 - val_acc: 0.1310\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 6s - loss: 2.4794 - acc: 0.1310 - val_loss: 2.1721 - val_acc: 0.2190\n",
      "Softmax acc: 21.90%\n",
      "Generating train features for SVM.."
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import svm\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import config\n",
    "import util\n",
    "\n",
    "fold_count = 1\n",
    "\n",
    "def tune(X_train, X_test, y_train, y_test):\n",
    "    Y_train = np_utils.to_categorical(y_train, config.nb_class)\n",
    "    Y_test = np_utils.to_categorical(y_test, config.nb_class)\n",
    "\n",
    "    model = util.load_alexnet_model(weights_path=config.alexnet_weights_path, nb_class=config.nb_class)\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=SGD(lr=1e-6, momentum=0.9),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    print \"Fine-tuning CNN..\"\n",
    "\n",
    "    hist = model.fit(X_train, Y_train,\n",
    "              nb_epoch=6000, batch_size=32,verbose=1,\n",
    "              validation_data=(X_test, Y_test))\n",
    "\n",
    "    util.save_history(hist,\"finetune567_fold\"+ str(fold_count),fold_count)\n",
    "\n",
    "    scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Softmax %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "    model.save_weights(\"models/finetune567_weights\"+ str(fold_count) +\".h5\")\n",
    "\n",
    "    model = None\n",
    "    model = util.load_svm_alex_model(weights_path=config.alexnet_weights_path, nb_class=config.nb_class)\n",
    "    print \"Generating train features for SVM..\"\n",
    "    svm_train = model.predict(X_train)\n",
    "    print svm_train.shape\n",
    "    print \"Generating test features for SVM..\"\n",
    "    svm_test = model.predict(X_test)\n",
    "    print svm_test.shape\n",
    "\n",
    "    print \"\\nTraining SVM..\"\n",
    "    clf = svm.SVC(kernel='linear', gamma=0.7, C=1.0)\n",
    "\n",
    "    clf.fit(svm_train, y_train.ravel())\n",
    "    #y_pred = clf.predict(test_data)\n",
    "    score = clf.score(svm_test, y_test.ravel())\n",
    "    print(\"SVM %s: %.2f%%\" % (\"acc: \", score*100))\n",
    "\n",
    "    y_pred = clf.predict(svm_test)\n",
    "\n",
    "    target_names = ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "     'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    "     'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    "     'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    "     'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    "     'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    "     'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    "     'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    "     'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    "     'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    "     'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    "     'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    "     'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    "     'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    "     'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    "     'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    "     'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    "     'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng']\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(classification_report(y_test, y_pred,target_names=target_names))\n",
    "    print(cm)\n",
    "\n",
    "    # Visualization of confusion matrix\n",
    "    #np.set_printoptions(precision=2)\n",
    "    #plt.figure()\n",
    "    #plot_confusion_matrix(cm)\n",
    "    #plt.show()\n",
    "\n",
    "    # Clear memory\n",
    "    X_train = None\n",
    "    Y_train = None\n",
    "    svm_train = None\n",
    "    svm_test = None\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_scores = 0\n",
    "\n",
    "    print \"Loading data..\"\n",
    "    data, labels, lz = util.load_data()\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    lz = np.array(lz)\n",
    "    print lz.shape\n",
    "    print \"Data loaded !\"\n",
    "\n",
    "    skf = StratifiedKFold(y=lz, n_folds=config.n_folds, shuffle=False)\n",
    "\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print \"Test train Shape: \"\n",
    "        print data[train].shape\n",
    "        print data[test].shape\n",
    "        print (\"Running Fold %d / %d\" % (i+1, config.n_folds))\n",
    "\n",
    "        scores = tune(data[train], data[test],labels[train], labels[test])\n",
    "        total_scores = total_scores + scores\n",
    "        fold_count = fold_count + 1\n",
    "    print(\"Average acc : %.2f%%\" % (total_scores/config.n_folds*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
