{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating SIFT descriptors. Number of images in ../dataset/food2/AngKuKueh is 200\n",
      "Calculating SIFT descriptors. Number of images in ../dataset/food2/AisKacang is 200\n",
      "Feature extration: 125.044704914 seconds\n",
      "Number of words  (128, 574883)\n",
      "('Training GMM of size', 32)\n",
      "GMM: 36.1494910717 seconds\n",
      "(128, 32)\n",
      "(128, 32)\n",
      "(32, 1)\n",
      "Encoding FV\n",
      "Fisher Vector: 156.892665863 seconds\n",
      "Training SVM\n",
      "(400,)\n",
      "SVM: 1.38238716125 seconds\n",
      "Acc for fold  1 =  0.99\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "[ 98 100   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "[100 100   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c46cf4ab4919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0mgmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgengmm_folder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mloadgmm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgenerate_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgengmm_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0mfisher_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mloadfv\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfisher_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfisher_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total: %s seconds\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtotal_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c46cf4ab4919>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(gmm, features)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mcorrect_predict_top5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Top-1 Accuracy: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Top-5 Accuracy: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop5\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mtotal_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtop1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "#Author: Jacob Gildenblat, 2014\n",
    "#License: you may use this for whatever you like \n",
    "import sys, glob, argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math, cv2\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import hickle as hkl\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from _vlfeat import *\n",
    "'''\n",
    "def dictionary(descriptors, N):\n",
    "    em = cv2.EM(N)\n",
    "    em.train(descriptors)\n",
    "\n",
    "    return np.float32(em.getMat(\"means\")), \\\n",
    "        np.float32(em.getMatVector(\"covs\")), np.float32(em.getMat(\"weights\"))[0]\n",
    "'''\n",
    "\n",
    "def dictionary(descriptors, N):\n",
    "    means, covs, priors, _ = vl_gmm(descriptors, N)\n",
    "    #save(\"means.gmm\", gmm.means_)\n",
    "    #save(\"covs.gmm\", gmm.covars_)\n",
    "    #save(\"weights.gmm\", gmm.weights_)\n",
    "    return means, covs, priors \n",
    "\n",
    "def image_descriptors(file):\n",
    "    img = cv2.imread(file, 0)\n",
    "    #img = cv2.resize(img, (256, 256))\n",
    "    img = np.array(img, 'f', order='F') # 'F' = column-major order!\n",
    "    img = np.array(img, 'float32')\n",
    "\n",
    "    f, descriptors = vl_sift(img,floatDescriptors=True, verbose=False) #0.7225 acc\n",
    "    #descriptor = cv2.DescriptorExtractor_create(\"OpponentSURF\")\n",
    "    #_ , descriptors = cv2.SIFT().detectAndCompute(img, None)\n",
    "    #descriptors = apply_pca(descriptors)\n",
    "    #f, descriptors = vl_dsift(img, fast=False, norm=True, step=100, floatDescriptors=True, verbose=False, size=5) #0.33 acc\n",
    "    #f, descriptors = vl_phow(img, verbose=False) #0.73125 128x128\n",
    "    descriptors = np.swapaxes(descriptors,0,1)\n",
    "    return descriptors\n",
    "\n",
    "def folder_descriptors(folder):\n",
    "    files = glob.glob(folder + \"/*.jpg\")\n",
    "    print \"Calculating SIFT descriptors. Number of images in \"+ folder +\" is \" + str(len(files))\n",
    "    return np.concatenate([image_descriptors(file) for file in files])\n",
    "\n",
    "def fisher_vector(samples, means, covs, w):\n",
    "    samples = np.swapaxes(samples,0,1)\n",
    "    fv = vl_fisher(samples, means, covs, w, fast=True, improved=True)\n",
    "    test = []\n",
    "    \n",
    "    for i in fv:\n",
    "        test = np.append(test , i[0])\n",
    "        \n",
    "    return test\n",
    "\n",
    "def apply_pca(image_descriptors):\n",
    "    pca = PCA(n_components=64)\n",
    "    return (pca.fit_transform(image_descriptors))\n",
    "\n",
    "def generate_gmm(input_folder, N):\n",
    "    loadfeature = False\n",
    "    \n",
    "    # start count execution time\n",
    "    start_time = time.time()\n",
    "    words = load_feature() if loadfeature else np.concatenate([folder_descriptors(folder) for folder in glob.glob(input_folder + '/*')]) \n",
    "    print(\"Feature extration: %s seconds\" % (time.time() - start_time))\n",
    "    hkl.dump(words, 'sift_feature.h5', mode='w')\n",
    "        \n",
    "    words = np.swapaxes(words,0,1)\n",
    "    print \"Number of words \", words.shape\n",
    "    print(\"Training GMM of size\", N)\n",
    "    start_time = time.time()\n",
    "    means, covs, weights = dictionary(words, N)\n",
    "    print(\"GMM: %s seconds\" % (time.time() - start_time))\n",
    "    \n",
    "    print means.shape\n",
    "    print covs.shape\n",
    "    print weights.shape\n",
    "\n",
    "    np.save(\"means.gmm\", means)\n",
    "    np.save(\"covs.gmm\", covs)\n",
    "    np.save(\"weights.gmm\", weights)\n",
    "    return means, covs, weights\n",
    "\n",
    "def get_fisher_vectors_from_folder(folder, gmm):\n",
    "    files = glob.glob(folder + \"/*.jpg\")\n",
    "    return np.float32([fisher_vector(image_descriptors(file), *gmm) for file in files])\n",
    "\n",
    "def fisher_features(folder, gmm):\n",
    "    print \"Encoding FV\"\n",
    "    folders = glob.glob(folder + \"/*\")\n",
    "    start_time = time.time()\n",
    "    features = {f : get_fisher_vectors_from_folder(f, gmm) for f in folders}\n",
    "    print(\"Fisher Vector: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "    f = open('sift_fv.pkl', 'wb')\n",
    "    pickle.dump(features, f)\n",
    "    #hkl.dump(features, \"sift_fv.h5\", mode='w')\n",
    "    \n",
    "    return features\n",
    "\n",
    "def train(gmm, features):\n",
    "    print \"Training SVM\"\n",
    "\n",
    "    X = np.concatenate(features.values())\n",
    "    y = np.concatenate([np.float32([i]*len(v)) for i,v in zip(range(0, len(features)), features.values())])\n",
    "    print y.shape\n",
    "    \n",
    "    k = 2\n",
    "    sfold = StratifiedKFold(y, n_folds=k)\n",
    "    \n",
    "    total_score = 0\n",
    "    fold_count = 1\n",
    "\n",
    "    for train_index, test_index in sfold:\n",
    "        train_data, test_data = X[train_index], X[test_index]\n",
    "        train_label, test_label = y[train_index], y[test_index]\n",
    "        start_time = time.time()\n",
    "        clf = svm.SVC(kernel='linear', C=1.0, probability=True)\n",
    "        clf.fit(train_data, train_label)\n",
    "        print(\"SVM: %s seconds\" % (time.time() - start_time))\n",
    "        \n",
    "        #print \"Saving SIFT SVM model..\"\n",
    "        #hkl.dump(clf, \"sift_svm_\"+ str(fold_count) +\".h5\")\n",
    "        \n",
    "        y_pred = clf.predict(test_data)\n",
    "    \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(test_label, y_pred)\n",
    "        np.set_printoptions(precision=2)\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        #print(cm)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm)\n",
    "\n",
    "        # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "        # in each class)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print('Normalized confusion matrix')\n",
    "        #print(cm_normalized)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        score = clf.score(test_data,test_label)\n",
    "        print \"Acc for fold \", fold_count, \"= \", score\n",
    "        \n",
    "        scores = clf.predict_proba(test_data)\n",
    "        n = 5\n",
    "        indices = np.argsort(scores)[:,:-n-1:-1]\n",
    "\n",
    "        # Get accuracy\n",
    "        top1 = 0.0\n",
    "        top5 = 0.0\n",
    " \n",
    "        correct_predict_top1 = np.zeros((100,), dtype=np.int)\n",
    "        correct_predict_top5 = np.zeros((100,), dtype=np.int)\n",
    "        \n",
    "        for image_index, index_list in enumerate(indices):\n",
    "            print test_label[image_index]\n",
    "            if test_label[image_index] == index_list[0]:\n",
    "                top1 += 1.0\n",
    "            if test_label[image_index] in index_list:\n",
    "                top5 += 1.0\n",
    "        \n",
    "        image_index = None    \n",
    "        index_list = None\n",
    "        start_index = 0\n",
    "        end_index = 99\n",
    "\n",
    "        for class_label in range(0,2):\n",
    "            for image_index in range(start_index,end_index+1):\n",
    "                if test_label[image_index] == indices[image_index][0]:\n",
    "                    correct_predict_top1[class_label] += 1\n",
    "                if test_label[image_index] in indices[image_index]:\n",
    "                    correct_predict_top5[class_label] += 1\n",
    "            start_index += 100\n",
    "            end_index += 100\n",
    "\n",
    "        objects = ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "         'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    "         'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    "         'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    "         'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    "         'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    "         'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    "         'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    "         'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    "         'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    "         'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    "         'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    "         'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    "         'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    "         'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    "         'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    "         'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    "         'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng']\n",
    "        y_pos = np.arange(len(objects))\n",
    "        performance = correct_predict_top1\n",
    "\n",
    "        rects1 = plt.bar(y_pos, performance)\n",
    "        plt.xticks(y_pos, objects, rotation='vertical')\n",
    "        plt.ylabel('Total true positive')\n",
    "        plt.title('Total true positive per sample')\n",
    "\n",
    "        autolabel(rects1)\n",
    "        plt.savefig('barchart_deep_feaures'+ str(fold_count) +'.png')\n",
    "        plt.show()\n",
    "\n",
    "        print correct_predict_top1\n",
    "        print correct_predict_top5\n",
    "\n",
    "        print('Top-1 Accuracy: ' + str(top1 / len(y_test) * 100.0) + '%')\n",
    "        print('Top-5 Accuracy: ' + str(top5 / len(y_test) * 100.0) + '%')\n",
    "        total_score = total_score + top1 / len(y_test)\n",
    "        fold_count = fold_count + 1\n",
    "\n",
    "    print \"Accuracy : \", total_score/k\n",
    "    return clf\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.jet):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(100)\n",
    "    plt.xticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    " 'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    " 'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    " 'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    " 'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    " 'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    " 'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    " 'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    " 'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    " 'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    " 'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    " 'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    " 'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    " 'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    " 'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    " 'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    " 'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    " 'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    " 'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    " 'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    " 'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    " 'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    " 'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    " 'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    " 'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    " 'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    " 'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    " 'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    " 'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    " 'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    " 'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    " 'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    " 'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    " 'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    " 'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def autolabel(rects):\n",
    "    # attach some text labels\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        plt.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '%d' % int(height),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "    \n",
    "def load_gmm(folder = \"\"): \n",
    "    print \"Loading GMM..\"\n",
    "    f = file(\"means.gmm.npy\",\"rb\")\n",
    "    means = np.load(f)\n",
    "    \n",
    "    f = file(\"covs.gmm.npy\",\"rb\")\n",
    "    covs = np.load(f)\n",
    "    \n",
    "    f = file(\"weights.gmm.npy\",\"rb\") \n",
    "    weights = np.load(f)\n",
    "    \n",
    "    return means, covs, weights\n",
    "\n",
    "def load_fv():\n",
    "    print \"Loading SIFT fisher vector..\"\n",
    "    with open('sift_fv.pkl', 'rb') as f:\n",
    "        fv = pickle.load(f)\n",
    "    #fv = hkl.load(\"sift_fv.h5\")\n",
    "    return fv\n",
    "\n",
    "def load_feature():\n",
    "    print \"Loading SIFT Feature..\"\n",
    "    with open('sift_feature.pkl', 'rb') as f:\n",
    "        feature = pickle.load(f)\n",
    "    #feature = hkl.load('sift_feature.pkl')\n",
    "    return feature\n",
    "\n",
    "number = 32\n",
    "working_folder = \"../dataset/food2\"\n",
    "gengmm_folder = \"../dataset/food2\"\n",
    "loadgmm = False\n",
    "loadfv = False\n",
    "\n",
    "total_time = time.time()\n",
    "  \n",
    "gmm = load_gmm(gengmm_folder) if loadgmm else generate_gmm(gengmm_folder, number)\n",
    "fisher_features = load_fv() if loadfv else fisher_features(working_folder, gmm)\n",
    "classifier = train(gmm, fisher_features)\n",
    "\n",
    "print(\"Total: %s seconds\" % (time.time() - total_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
