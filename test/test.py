import os
import h5py
from sklearn.cross_validation import StratifiedKFold
from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential, Model
from keras.layers import Flatten, Dense, Dropout, Reshape, Permute, Activation, \
Input, merge
from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D

from convnetskeras.customlayers import convolution2Dgroup, crosschannelnormalization, \
splittensor, Softmax4D

from keras.layers import Convolution2D, MaxPooling2D
from keras.optimizers import SGD
from keras.utils import np_utils
import numpy as np

try:
    import cPickle as pickle
except:
    import pickle

batch_size = 32
nb_classes = 100
nb_epoch = 100
data_augmentation = False

# input image dimensions
img_rows, img_cols = 227,227
img_channels = 3

def load_data():
    # load your data using this function
    f = open("../dataset/myfood10-227.pkl", 'rb')

    d = pickle.load(f)
    data = d['trainFeatures']
    labels = d['trainLabels']
    lz = d['labels']
    data = data.reshape(data.shape[0], 3, 227, 227)
    #data = data.transpose(0, 2, 3, 1)

    return data,labels,lz

def create_model(weights_path=None, heatmap=False):

    inputs = Input(shape=(3,227,227))

    conv_1 = Convolution2D(96, 11, 11,subsample=(4,4),activation='relu',
                           name='conv_1')(inputs)

    conv_2 = MaxPooling2D((3, 3), strides=(2,2))(conv_1)
    conv_2 = crosschannelnormalization(name="convpool_1")(conv_2)
    conv_2 = ZeroPadding2D((2,2))(conv_2)
    conv_2 = merge([
        Convolution2D(128,5,5,activation="relu",name='conv_2_'+str(i+1))(
            splittensor(ratio_split=2,id_split=i)(conv_2)
        ) for i in range(2)], mode='concat',concat_axis=1,name="conv_2")

    conv_3 = MaxPooling2D((3, 3), strides=(2, 2))(conv_2)
    conv_3 = crosschannelnormalization()(conv_3)
    conv_3 = ZeroPadding2D((1,1))(conv_3)
    conv_3 = Convolution2D(384,3,3,activation='relu',name='conv_3')(conv_3)

    conv_4 = ZeroPadding2D((1,1))(conv_3)
    conv_4 = merge([
        Convolution2D(192,3,3,activation="relu",name='conv_4_'+str(i+1))(
            splittensor(ratio_split=2,id_split=i)(conv_4)
        ) for i in range(2)], mode='concat',concat_axis=1,name="conv_4")

    conv_5 = ZeroPadding2D((1,1))(conv_4)
    conv_5 = merge([
        Convolution2D(128,3,3,activation="relu",name='conv_5_'+str(i+1))(
            splittensor(ratio_split=2,id_split=i)(conv_5)
        ) for i in range(2)], mode='concat',concat_axis=1,name="conv_5")

    dense_1 = MaxPooling2D((3, 3), strides=(2,2),name="convpool_5")(conv_5)


    dense_1 = Flatten(name="flatten")(dense_1)
    dense_1 = Dense(4096, activation='relu',name='dense_1')(dense_1)
    dense_2 = Dropout(0.5)(dense_1)
    dense_2 = Dense(4096, activation='relu',name='dense_2')(dense_2)
    dense_3 = Dropout(0.5)(dense_2)
    dense_3 = Dense(nb_classes,name='dense_3')(dense_3)
    prediction = Activation("softmax",name="softmax")(dense_3)


    model = Model(input=inputs, output=prediction)

    if weights_path:
        model.load_weights(weights_path)

    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy',
                  optimizer=sgd,
                  metrics=['accuracy'])

    return model

def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):

    print "Training.."

    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)
    if not data_augmentation:
        print('Not using data augmentation.')
        #model.fit(X_train, Y_train,
        #          batch_size=batch_size,
        #          nb_epoch=nb_epoch,
        #          validation_data=(X_test, Y_test),
        #          shuffle=True)
        
        model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size)

        scores = model.evaluate(X_test, Y_test)
        print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))


    else:
        print('Using real-time data augmentation.')

        # this will do preprocessing and realtime data augmentation
        datagen = ImageDataGenerator(
            featurewise_center=True,
            featurewise_std_normalization=True,
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            horizontal_flip=True)

        # compute quantities required for featurewise normalization
        # (std, mean, and principal components if ZCA whitening is applied)
        datagen.fit(X_train)

        # fit the model on the batches generated by datagen.flow()
        model.fit_generator(datagen.flow(X_train, Y_train,
                            batch_size=batch_size),
                            samples_per_epoch=X_train.shape[0],
                            nb_epoch=nb_epoch)

        scores = model.evaluate(X_test, Y_test)
        print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

    return scores

if __name__ == "__main__":
    n_folds = 2
    cvscores = []
    data_augmentation = True
    print "Loading data.."
    data, labels, lz = load_data()
    print "Data loaded !"
    data = data.astype('float32')
    data /= 255
    lz = np.array(lz)


    skf = StratifiedKFold(y=lz, n_folds=n_folds, shuffle=True)

    for i, (train, test) in enumerate(skf):
        print ("Running Fold", i+1, "/", n_folds)
        model = None # Clearing the NN.
        model = create_model(weights_path=None, heatmap=False)
        scores = train_and_evaluate_model(model, data[train], labels[train], data[test], labels[test])
        cvscores.append(scores[1] * 100)

    print("Average acc : %.2f%% (+/- %.2f%%)" % (np.mean(cvscores), np.std(cvscores)))
