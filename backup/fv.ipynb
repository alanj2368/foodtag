{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating SIFT descriptors. Number of images in food/food10_test_gmm/AisKacang is 100\n",
      "Calculating SIFT descriptors. Number of images in food/food10_test_gmm/AngKuKueh is 100\n",
      "Calculating SIFT descriptors. Number of images in food/food10_test_gmm/ApamBalik is 100\n",
      "Calculating SIFT descriptors. Number of images in food/food10_test_gmm/Asamlaksa is 100\n",
      "Number of words  (163004, 128)\n",
      "('Training GMM of size', 5)\n",
      "(2, 128)\n",
      "(2, 128, 128)\n",
      "(2,)\n",
      "Encoding FV\n",
      "Training SVM\n",
      "Score for fold  1 =  0.8\n",
      "Score for fold  2 =  0.78\n",
      "Accuracy :  0.79\n"
     ]
    }
   ],
   "source": [
    "#Author: Jacob Gildenblat, 2014\n",
    "#License: you may use this for whatever you like \n",
    "import sys, glob, argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math, cv2\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "def dictionary(descriptors, N):\n",
    "    em = cv2.EM(N)\n",
    "    em.train(descriptors)\n",
    "\n",
    "    return np.float32(em.getMat(\"means\")), \\\n",
    "        np.float32(em.getMatVector(\"covs\")), np.float32(em.getMat(\"weights\"))[0]\n",
    "\n",
    "def image_descriptors(file):\n",
    "\timg = cv2.imread(file, 0)\n",
    "\timg = cv2.resize(img, (256, 256))\n",
    "\t_ , descriptors = cv2.SIFT().detectAndCompute(img, None)\n",
    "\treturn descriptors\n",
    "\n",
    "def folder_descriptors(folder):\n",
    "    files = glob.glob(folder + \"/*.jpg\")\n",
    "    print \"Calculating SIFT descriptors. Number of images in \"+ folder +\" is \" + str(len(files))\n",
    "    return np.concatenate([image_descriptors(file) for file in files])\n",
    "\n",
    "def likelihood_moment(x, ytk, moment):\t\n",
    "\tx_moment = np.power(np.float32(x), moment) if moment > 0 else np.float32([1])\n",
    "\treturn x_moment * ytk\n",
    "\n",
    "def likelihood_statistics(samples, means, covs, weights):\n",
    "\tgaussians, s0, s1,s2 = {}, {}, {}, {}\n",
    "\tsamples = zip(range(0, len(samples)), samples)\n",
    "\t\n",
    "\tg = [multivariate_normal(mean=means[k], cov=covs[k]) for k in range(0, len(weights)) ]\n",
    "\tfor index, x in samples:\n",
    "\t\tgaussians[index] = np.array([g_k.pdf(x) for g_k in g])\n",
    "\n",
    "\tfor k in range(0, len(weights)):\n",
    "\t\ts0[k], s1[k], s2[k] = 0, 0, 0\n",
    "\t\tfor index, x in samples:\n",
    "\t\t\tprobabilities = np.multiply(gaussians[index], weights)\n",
    "\t\t\tprobabilities = probabilities / np.sum(probabilities)\n",
    "\t\t\ts0[k] = s0[k] + likelihood_moment(x, probabilities[k], 0)\n",
    "\t\t\ts1[k] = s1[k] + likelihood_moment(x, probabilities[k], 1)\n",
    "\t\t\ts2[k] = s2[k] + likelihood_moment(x, probabilities[k], 2)\n",
    "\n",
    "\treturn s0, s1, s2\n",
    "\n",
    "def fisher_vector_weights(s0, s1, s2, means, covs, w, T):\n",
    "\treturn np.float32([((s0[k] - T * w[k]) / np.sqrt(w[k]) ) for k in range(0, len(w))])\n",
    "\n",
    "def fisher_vector_means(s0, s1, s2, means, sigma, w, T):\n",
    "\treturn np.float32([(s1[k] - means[k] * s0[k]) / (np.sqrt(w[k] * sigma[k])) for k in range(0, len(w))])\n",
    "\n",
    "def fisher_vector_sigma(s0, s1, s2, means, sigma, w, T):\n",
    "\treturn np.float32([(s2[k] - 2 * means[k]*s1[k]  + (means[k]*means[k] - sigma[k]) * s0[k]) / (np.sqrt(2*w[k])*sigma[k])  for k in range(0, len(w))])\n",
    "\n",
    "def normalize(fisher_vector):\n",
    "\tv = np.sqrt(abs(fisher_vector)) * np.sign(fisher_vector)\n",
    "\treturn v / np.sqrt(np.dot(v, v))\n",
    "\n",
    "def fisher_vector(samples, means, covs, w):\n",
    "    s0, s1, s2 =  likelihood_statistics(samples, means, covs, w)\n",
    "    T = samples.shape[0]\n",
    "    covs = np.float32([np.diagonal(covs[k]) for k in range(0, covs.shape[0])])\n",
    "    a = fisher_vector_weights(s0, s1, s2, means, covs, w, T)\n",
    "    b = fisher_vector_means(s0, s1, s2, means, covs, w, T)\n",
    "    c = fisher_vector_sigma(s0, s1, s2, means, covs, w, T)\n",
    "    fv = np.concatenate([np.concatenate(a), np.concatenate(b), np.concatenate(c)])\n",
    "    fv = normalize(fv)\n",
    "    return fv\n",
    "\n",
    "def generate_gmm(input_folder, N):\n",
    "    words = np.concatenate([folder_descriptors(folder) for folder in glob.glob(input_folder + '/*')]) \n",
    "    print \"Number of words \", words.shape\n",
    "    print(\"Training GMM of size\", N)\n",
    "    means, covs, weights = dictionary(words, N)\n",
    "    #Throw away gaussians with weights that are too small:\n",
    "    th = 1.0 / N\n",
    "    means = np.float32([m for k,m in zip(range(0, len(weights)), means) if weights[k] > th])\n",
    "    covs = np.float32([m for k,m in zip(range(0, len(weights)), covs) if weights[k] > th])\n",
    "    weights = np.float32([m for k,m in zip(range(0, len(weights)), weights) if weights[k] > th])\n",
    "    \n",
    "    print means.shape\n",
    "    print covs.shape\n",
    "    print weights.shape\n",
    "\n",
    "    np.save(\"means.gmm\", means)\n",
    "    np.save(\"covs.gmm\", covs)\n",
    "    np.save(\"weights.gmm\", weights)\n",
    "    return means, covs, weights\n",
    "\n",
    "def get_fisher_vectors_from_folder(folder, gmm):\n",
    "\tfiles = glob.glob(folder + \"/*.jpg\")\n",
    "\treturn np.float32([fisher_vector(image_descriptors(file), *gmm) for file in files])\n",
    "\n",
    "def fisher_features(folder, gmm):\n",
    "    print \"Encoding FV\"\n",
    "    folders = glob.glob(folder + \"/*\")\n",
    "    features = {f : get_fisher_vectors_from_folder(f, gmm) for f in folders}\n",
    "    with open( 'fv.pkl', 'wb') as f:\n",
    "        pickle.dump(features, f, pickle.HIGHEST_PROTOCOL)\n",
    "    return features\n",
    "\n",
    "def train(gmm, features):\n",
    "    print \"Training SVM\"\n",
    "    X = np.concatenate(features.values())\n",
    "    y = np.concatenate([np.float32([i]*len(v)) for i,v in zip(range(0, len(features)), features.values())])\n",
    "    \n",
    "    k = 2\n",
    "    sfold = StratifiedKFold(y, n_folds=k)\n",
    "    \n",
    "    total_score = 0\n",
    "    fold_count = 1\n",
    "\n",
    "    for train_index, test_index in sfold:\n",
    "        train_data, test_data = X[train_index], X[test_index]\n",
    "        train_label, test_label = y[train_index], y[test_index]\n",
    "        \n",
    "        clf = svm.SVC(kernel='rbf', gamma=0.7, C=1.0)\n",
    "        clf.fit(train_data, train_label)\n",
    "        \n",
    "        y_pred = clf.predict(test_data)\n",
    "    \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(test_label, y_pred)\n",
    "        np.set_printoptions(precision=2)\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        #print(cm)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix4(cm)\n",
    "\n",
    "        # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "        # in each class)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print('Normalized confusion matrix')\n",
    "        #print(cm_normalized)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix4(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        score = clf.score(test_data,test_label)\n",
    "        print \"Score for fold \", fold_count, \"= \", score\n",
    "        total_score = total_score + score\n",
    "        fold_count = fold_count + 1\n",
    "\n",
    "    print \"Accuracy : \", total_score/k\n",
    "    return clf\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.jet):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(100)\n",
    "    plt.xticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    " 'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    " 'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    " 'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    " 'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    " 'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    " 'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    " 'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    " 'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    " 'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    " 'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    " 'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    " 'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    " 'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    " 'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    " 'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    " 'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    " 'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    " 'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    " 'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    " 'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    " 'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    " 'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    " 'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    " 'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    " 'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    " 'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    " 'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    " 'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    " 'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    " 'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    " 'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    " 'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    " 'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    " 'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot_confusion_matrix4(cm, title='Confusion matrix', cmap=plt.cm.jet):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(4)\n",
    "    plt.xticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa'])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    \n",
    "def load_gmm(folder = \"\"):  \n",
    "    f = file(\"means.gmm.npy\",\"rb\")\n",
    "    means = np.load(f)\n",
    "    \n",
    "    f = file(\"covs.gmm.npy\",\"rb\")\n",
    "    covs = np.load(f)\n",
    "    \n",
    "    f = file(\"weights.gmm.npy\",\"rb\")\n",
    "    weights = np.load(f)\n",
    "    \n",
    "    return means, covs, weights\n",
    "\n",
    "def load_fv():\n",
    "    with open('fv.pkl', 'rb') as f:\n",
    "        fv = pickle.load(f)\n",
    "    return fv\n",
    "\n",
    "number = 5\n",
    "working_folder = \"food/food10_test\"\n",
    "gengmm_folder = \"food/food10_test_gmm\"\n",
    "loadgmm = False\n",
    "loadfv = False\n",
    "\n",
    "gmm = load_gmm(gengmm_folder) if loadgmm else generate_gmm(gengmm_folder, number)\n",
    "fisher_features = load_fv() if loadfv else fisher_features(working_folder, gmm)\n",
    "classifier = train(gmm, fisher_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
