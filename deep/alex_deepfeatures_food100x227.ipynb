{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is disabled, cuDNN 5105)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "Data loaded !\n",
      "(15000, 3, 227, 227)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/machine/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode='max')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 3, 227, 227)\n",
      "Test train splitted !\n",
      "deep features extracted (x,4096)\n",
      "Training SVM..\n",
      "acc: : 22.24%\n",
      "Training CNN..\n",
      "Epoch 1/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.6085 - acc: 0.0094     \n",
      "Epoch 2/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.6012 - acc: 0.0110     \n",
      "Epoch 3/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5945 - acc: 0.0134     \n",
      "Epoch 4/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5882 - acc: 0.0193     \n",
      "Epoch 5/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5822 - acc: 0.0239     \n",
      "Epoch 6/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5765 - acc: 0.0292     \n",
      "Epoch 7/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5709 - acc: 0.0312     \n",
      "Epoch 8/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5655 - acc: 0.0355     \n",
      "Epoch 9/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5602 - acc: 0.0382     \n",
      "Epoch 10/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5550 - acc: 0.0402     \n",
      "Epoch 11/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5498 - acc: 0.0430     \n",
      "Epoch 12/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5447 - acc: 0.0457     \n",
      "Epoch 13/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5397 - acc: 0.0500     \n",
      "Epoch 14/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5347 - acc: 0.0535     \n",
      "Epoch 15/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5298 - acc: 0.0569     \n",
      "Epoch 16/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5249 - acc: 0.0615     \n",
      "Epoch 17/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5200 - acc: 0.0633     \n",
      "Epoch 18/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5152 - acc: 0.0675     \n",
      "Epoch 19/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5104 - acc: 0.0705     \n",
      "Epoch 20/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5057 - acc: 0.0746     \n",
      "Epoch 21/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.5010 - acc: 0.0779     \n",
      "Epoch 22/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4963 - acc: 0.0800     \n",
      "Epoch 23/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4916 - acc: 0.0817     \n",
      "Epoch 24/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4870 - acc: 0.0849     \n",
      "Epoch 25/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4824 - acc: 0.0875     \n",
      "Epoch 26/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4779 - acc: 0.0913     \n",
      "Epoch 27/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4734 - acc: 0.0928     \n",
      "Epoch 28/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4689 - acc: 0.0937     \n",
      "Epoch 29/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4644 - acc: 0.0947     \n",
      "Epoch 30/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4600 - acc: 0.0947     \n",
      "Epoch 31/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4556 - acc: 0.0979     \n",
      "Epoch 32/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4512 - acc: 0.0997     \n",
      "Epoch 33/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4469 - acc: 0.1024     \n",
      "Epoch 34/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4426 - acc: 0.1041     \n",
      "Epoch 35/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4383 - acc: 0.1038     \n",
      "Epoch 36/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4340 - acc: 0.1041     \n",
      "Epoch 37/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4298 - acc: 0.1059     \n",
      "Epoch 38/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4256 - acc: 0.1073     \n",
      "Epoch 39/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4214 - acc: 0.1100     \n",
      "Epoch 40/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4173 - acc: 0.1104     \n",
      "Epoch 41/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4132 - acc: 0.1128     \n",
      "Epoch 42/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4091 - acc: 0.1151     \n",
      "Epoch 43/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4050 - acc: 0.1152     \n",
      "Epoch 44/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.4010 - acc: 0.1142     \n",
      "Epoch 45/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3970 - acc: 0.1162     \n",
      "Epoch 46/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3930 - acc: 0.1172     \n",
      "Epoch 47/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3891 - acc: 0.1175     \n",
      "Epoch 48/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3851 - acc: 0.1179     \n",
      "Epoch 49/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3812 - acc: 0.1179     \n",
      "Epoch 50/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3774 - acc: 0.1186     \n",
      "Epoch 51/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3735 - acc: 0.1195     \n",
      "Epoch 52/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3696 - acc: 0.1231     \n",
      "Epoch 53/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3658 - acc: 0.1213     \n",
      "Epoch 54/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3620 - acc: 0.1221     \n",
      "Epoch 55/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3583 - acc: 0.1232     \n",
      "Epoch 56/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3546 - acc: 0.1247     \n",
      "Epoch 57/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3509 - acc: 0.1246     \n",
      "Epoch 58/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3472 - acc: 0.1253     \n",
      "Epoch 59/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3435 - acc: 0.1261     \n",
      "Epoch 60/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3398 - acc: 0.1267     \n",
      "Epoch 61/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3362 - acc: 0.1272     \n",
      "Epoch 62/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3326 - acc: 0.1275     \n",
      "Epoch 63/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3290 - acc: 0.1292     \n",
      "Epoch 64/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3255 - acc: 0.1277     \n",
      "Epoch 65/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3219 - acc: 0.1293     \n",
      "Epoch 66/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3184 - acc: 0.1292     \n",
      "Epoch 67/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3149 - acc: 0.1290     \n",
      "Epoch 68/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3115 - acc: 0.1303     \n",
      "Epoch 69/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3080 - acc: 0.1301     \n",
      "Epoch 70/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3046 - acc: 0.1318     \n",
      "Epoch 71/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.3012 - acc: 0.1315     \n",
      "Epoch 72/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2978 - acc: 0.1323     \n",
      "Epoch 73/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2944 - acc: 0.1316     \n",
      "Epoch 74/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2911 - acc: 0.1324     \n",
      "Epoch 75/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2877 - acc: 0.1335     \n",
      "Epoch 76/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2844 - acc: 0.1335     \n",
      "Epoch 77/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2811 - acc: 0.1339     \n",
      "Epoch 78/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2779 - acc: 0.1360     \n",
      "Epoch 79/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2746 - acc: 0.1354     \n",
      "Epoch 80/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2714 - acc: 0.1343     \n",
      "Epoch 81/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2682 - acc: 0.1359     \n",
      "Epoch 82/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2650 - acc: 0.1366     \n",
      "Epoch 83/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2618 - acc: 0.1361     \n",
      "Epoch 84/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2586 - acc: 0.1377     \n",
      "Epoch 85/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2555 - acc: 0.1379     \n",
      "Epoch 86/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2524 - acc: 0.1388     \n",
      "Epoch 87/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2493 - acc: 0.1392     \n",
      "Epoch 88/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2462 - acc: 0.1395     \n",
      "Epoch 89/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2431 - acc: 0.1385     \n",
      "Epoch 90/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2400 - acc: 0.1403     \n",
      "Epoch 91/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2370 - acc: 0.1404     \n",
      "Epoch 92/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2340 - acc: 0.1405     \n",
      "Epoch 93/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2310 - acc: 0.1405     \n",
      "Epoch 94/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2280 - acc: 0.1413     \n",
      "Epoch 95/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2250 - acc: 0.1406     \n",
      "Epoch 96/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2221 - acc: 0.1421     \n",
      "Epoch 97/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2191 - acc: 0.1413     \n",
      "Epoch 98/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2162 - acc: 0.1427     \n",
      "Epoch 99/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2133 - acc: 0.1431     \n",
      "Epoch 100/100\n",
      "15000/15000 [==============================] - 1s - loss: 4.2104 - acc: 0.1432     \n",
      "acc: 12.52%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode='max')\n",
      "/home/machine/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode='max')\n",
      "/home/machine/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "           AisKacang       0.14      0.06      0.08        51\n",
      "           AngKuKueh       0.00      0.00      0.00        45\n",
      "           ApamBalik       0.29      0.04      0.07        52\n",
      "           Asamlaksa       0.00      0.00      0.00        48\n",
      "              Bahulu       0.17      0.91      0.28        55\n",
      "           Bakkukteh       0.50      0.08      0.14        48\n",
      "      BananaLeafRice       0.12      0.14      0.13        49\n",
      "             Bazhang       0.08      0.02      0.03        47\n",
      "         BeefRendang       0.15      0.76      0.25        42\n",
      "           BingkaUbi       0.08      0.24      0.12        38\n",
      "         Buburchacha       0.62      0.10      0.17        52\n",
      "          Buburpedas       0.13      0.74      0.22        42\n",
      "              Capati       0.06      0.26      0.09        39\n",
      "              Cendol       0.00      0.00      0.00        58\n",
      "         ChaiTowKuay       0.48      0.26      0.34        62\n",
      "        CharKuehTiao       0.08      0.31      0.13        45\n",
      "             CharSiu       0.15      0.21      0.18        47\n",
      "       CheeCheongFun       0.15      0.66      0.25        47\n",
      "           ChiliCrab       0.00      0.00      0.00        55\n",
      "           Chweekueh       0.14      0.35      0.20        40\n",
      "         ClayPotRice       0.03      0.02      0.03        44\n",
      "          CucurUdang       0.00      0.00      0.00        44\n",
      "          CurryLaksa       0.00      0.00      0.00        52\n",
      "           CurryPuff       0.00      0.00      0.00        39\n",
      "               Dodol       0.06      0.29      0.09        38\n",
      "              Durian       0.05      0.26      0.08        31\n",
      "         DurianCrepe       0.00      0.00      0.00        52\n",
      "       FishHeadCurry       0.17      0.02      0.04        46\n",
      "               Guava       0.03      0.04      0.03        48\n",
      "HainaneseChickenRice       0.00      0.00      0.00        58\n",
      "          HokkienMee       0.00      0.00      0.00        53\n",
      "            Huatkuih       0.24      0.19      0.21        43\n",
      "           IkanBakar       0.13      0.16      0.15        43\n",
      "            Kangkung       0.29      0.21      0.24        58\n",
      "           KayaToast       0.12      0.07      0.09        54\n",
      "            Keklapis       0.00      0.00      0.00        60\n",
      "             Ketupat       0.12      0.21      0.15        42\n",
      "           KuihDadar       0.00      0.00      0.00        48\n",
      "           KuihLapis       0.31      0.09      0.14        57\n",
      "        KuihSeriMuka       0.00      0.00      0.00        50\n",
      "             Langsat       0.00      0.00      0.00        60\n",
      "               Lekor       0.08      0.04      0.06        46\n",
      "              Lemang       0.21      0.25      0.23        48\n",
      "         LepatPisang       0.08      0.02      0.03        52\n",
      "              LorMee       0.20      0.02      0.03        54\n",
      "        Maggi goreng       0.33      0.44      0.38        48\n",
      "          Mangosteen       0.09      0.02      0.03        49\n",
      "           MeeGoreng       0.00      0.00      0.00        54\n",
      "         MeeHoonKueh       0.09      0.02      0.03        48\n",
      "         MeeHoonSoup       0.06      0.07      0.07        44\n",
      "             MeeJawa       0.17      0.26      0.20        47\n",
      "            MeeRebus       0.33      0.02      0.04        51\n",
      "            MeeRojak       0.00      0.00      0.00        57\n",
      "             MeeSiam       0.00      0.00      0.00        62\n",
      "            Murtabak       0.33      0.02      0.04        52\n",
      "             Murukku       0.20      0.02      0.04        52\n",
      "   NasiGorengKampung       1.00      0.03      0.07        59\n",
      "           NasiImpit       0.25      0.02      0.03        55\n",
      "          Nasikandar       0.20      0.24      0.22        55\n",
      "           Nasilemak       0.00      0.00      0.00        50\n",
      "         Nasipattaya       0.00      0.00      0.00        48\n",
      "          Ondehondeh       0.11      0.04      0.06        51\n",
      "            Otakotak       0.13      0.24      0.17        49\n",
      "      OysterOmelette       0.25      0.16      0.20        49\n",
      "              PanMee       0.00      0.00      0.00        49\n",
      "       PineappleTart       0.00      0.00      0.00        61\n",
      "        PisangGoreng       0.11      0.36      0.16        44\n",
      "              Popiah       0.00      0.00      0.00        65\n",
      "            PrawnMee       0.00      0.00      0.00        59\n",
      "         Prawnsambal       1.00      0.02      0.03        58\n",
      "                Puri       0.50      0.02      0.04        48\n",
      "           PutuMayam       0.04      0.02      0.03        48\n",
      "          PutuPiring       0.00      0.00      0.00        49\n",
      "            Rambutan       0.00      0.00      0.00        47\n",
      "               Rojak       0.20      0.45      0.28        56\n",
      "           RotiCanai       0.00      0.00      0.00        54\n",
      "            RotiJala       0.32      0.11      0.17        53\n",
      "            RotiJohn       0.16      0.53      0.25        55\n",
      "            RotiNaan       0.00      0.00      0.00        60\n",
      "          RotiTissue       0.14      0.05      0.08        39\n",
      "         SambalPetai       0.00      0.00      0.00        58\n",
      "         SambalUdang       0.22      0.11      0.14        46\n",
      "               Satay       0.14      0.40      0.20        50\n",
      "          Sataycelup       0.30      0.16      0.21        49\n",
      "            SeriMuka       0.28      0.42      0.34        48\n",
      "            SotoAyam       0.09      0.27      0.14        41\n",
      "     TandooriChicken       0.00      0.00      0.00        45\n",
      "            TangYuan       0.00      0.00      0.00        54\n",
      "           TauFooFah       0.14      0.02      0.03        56\n",
      "         TauhuSumbat       0.06      0.13      0.09        53\n",
      "              Thosai       0.05      0.60      0.09        42\n",
      "          TomYumSoup       0.10      0.76      0.18        41\n",
      "               Wajik       0.00      0.00      0.00        55\n",
      "           WanTanMee       0.03      0.02      0.03        48\n",
      "             WaTanHo       0.00      0.00      0.00        58\n",
      "              Wonton       0.00      0.00      0.00        57\n",
      "             YamCake       0.00      0.00      0.00        52\n",
      "           YongTauFu       0.37      0.28      0.32        47\n",
      "             Youtiao       0.00      0.00      0.00        46\n",
      "             Yusheng       0.12      0.02      0.04        47\n",
      "\n",
      "         avg / total       0.14      0.13      0.08      5000\n",
      "\n",
      "[[ 3  0  0 ...,  0  0  1]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  2 ...,  0  0  0]\n",
      " ..., \n",
      " [ 0  0  0 ..., 13  0  0]\n",
      " [ 0  0  0 ...,  1  0  0]\n",
      " [ 0  0  0 ...,  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Flatten, Dense, Dropout, Reshape, Permute, Activation, \\\n",
    "    Input, merge\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from convnetskeras.customlayers import convolution2Dgroup, crosschannelnormalization, \\\n",
    "    splittensor, Softmax4D\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.utils import np_utils\n",
    "from sklearn import svm\n",
    "\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "\n",
    "import util\n",
    "import config\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# path to the model weights file.\n",
    "weights_path = '../dataset/alexnet_weights.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "img_width, img_height = 224, 224\n",
    "nb_train_samples = 1500\n",
    "nb_validation_samples = 500\n",
    "nb_class = 100\n",
    "nb_epoch = 40\n",
    "\n",
    "def load_data():\n",
    "    # load your data using this function\n",
    "    f = open(\"../dataset/myfood100-227.pkl\", 'rb')\n",
    "    d = pickle.load(f)\n",
    "    data = d['trainFeatures']\n",
    "    labels = d['trainLabels']\n",
    "    lz = d['labels']\n",
    "    data = data.reshape(data.shape[0], 3, 227, 227)\n",
    "    #data = data.transpose(0, 2, 3, 1)\n",
    "\n",
    "    return data,labels,lz\n",
    "\n",
    "def get_top_model_for_alexnet(nb_class=None, shape=None, W_regularizer=False, weights_file_path=None, input=None, output=None):\n",
    "    if not output:\n",
    "        inputs = Input(shape=shape)\n",
    "\n",
    "    dense_3 = Dense(nb_class,name='dense_3')(inputs)\n",
    "    predictions = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "    model = Model(input=input or inputs, output=predictions)\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_model(nb_class, weights_path=None):\n",
    "\n",
    "    inputs = Input(shape=(3,227,227))\n",
    "\n",
    "    conv_1 = Convolution2D(96, 11, 11,subsample=(4,4),activation='relu',\n",
    "                           name='conv_1')(inputs)\n",
    "\n",
    "    conv_2 = MaxPooling2D((3, 3), strides=(2,2))(conv_1)\n",
    "    conv_2 = crosschannelnormalization(name=\"convpool_1\")(conv_2)\n",
    "    conv_2 = ZeroPadding2D((2,2))(conv_2)\n",
    "    conv_2 = merge([\n",
    "        Convolution2D(128,5,5,activation=\"relu\",name='conv_2_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_2)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_2\")\n",
    "\n",
    "    conv_3 = MaxPooling2D((3, 3), strides=(2, 2))(conv_2)\n",
    "    conv_3 = crosschannelnormalization()(conv_3)\n",
    "    conv_3 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_3 = Convolution2D(384,3,3,activation='relu',name='conv_3')(conv_3)\n",
    "\n",
    "    conv_4 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_4 = merge([\n",
    "        Convolution2D(192,3,3,activation=\"relu\",name='conv_4_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_4)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_4\")\n",
    "\n",
    "    conv_5 = ZeroPadding2D((1,1))(conv_4)\n",
    "    conv_5 = merge([\n",
    "        Convolution2D(128,3,3,activation=\"relu\",name='conv_5_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_5)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_5\")\n",
    "\n",
    "    conv_5 = MaxPooling2D((3, 3), strides=(2,2),name=\"convpool_5\")(conv_5)\n",
    "\n",
    "\n",
    "\n",
    "    dense_1 = Flatten(name=\"flatten\")(conv_5)\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1')(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2')(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(nb_class,name='dense_3')(dense_3)\n",
    "    prediction = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "\n",
    "\n",
    "    base_model = Model(input=inputs, output=prediction)\n",
    "\n",
    "    if weights_path:\n",
    "        base_model.load_weights(weights_path)\n",
    "\n",
    "    base_model = Model(input=inputs, output=dense_2)\n",
    "\n",
    "    return base_model\n",
    "\n",
    "\n",
    "\n",
    "def save_bottlebeck_features(X_train, X_test, y_train, y_test):\n",
    "    model = load_model(nb_class=nb_class, weights_path=weights_path)\n",
    "    '''\n",
    "    j = 0\n",
    "    for i in X_train:\n",
    "        temp = X_train[j]\n",
    "        temp = temp[None, ...]\n",
    "\n",
    "        bottleneck_features_train.append(model.predict(temp, batch_size=32)[0])\n",
    "        j+1\n",
    "    bottleneck_features_train = np.array(bottleneck_features_train)\n",
    "    np.save(open('alex_bottleneck_features_train.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "    j = 0\n",
    "    for i in X_test:\n",
    "        temp = X_train[j]\n",
    "        temp = temp[None, ...]\n",
    "        bottleneck_features_validation.append(model.predict(temp, batch_size=32)[0])\n",
    "        j+1\n",
    "    bottleneck_features_validation = np.array(bottleneck_features_validation)\n",
    "    np.save(open('alex_bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)\n",
    "    '''\n",
    "    bottleneck_features_train = model.predict(X_train, batch_size=32)\n",
    "    np.save(open('alex_bottleneck_features_train.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "\n",
    "    bottleneck_features_validation = model.predict(X_test, batch_size=32)\n",
    "    np.save(open('alex_bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)\n",
    "    print \"deep features extracted (x,4096)\"\n",
    "\n",
    "\n",
    "def train_top_model(y_train, y_test):\n",
    "    X_train = np.load(open('alex_bottleneck_features_train.npy' , 'rb'))\n",
    "    X_test = np.load(open('alex_bottleneck_features_validation.npy', 'rb'))\n",
    "\n",
    "    #svm_train_data = X_train.reshape(nb_train_samples,9216)\n",
    "    #svm_test_data = X_test.reshape(nb_validation_samples,9216)\n",
    "\n",
    "    print \"Training SVM..\"\n",
    "    clf = svm.SVC(kernel='rbf', gamma=0.7, C=1.0)\n",
    "\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    #y_pred = clf.predict(test_data)\n",
    "    score = clf.score(X_test, y_test.ravel())\n",
    "    print(\"%s: %.2f%%\" % (\"acc: \", score*100))\n",
    "\n",
    "\n",
    "    print \"Training CNN..\"\n",
    "    y_train = np_utils.to_categorical(y_train, nb_class)\n",
    "    y_test = np_utils.to_categorical(y_test, nb_class)\n",
    "\n",
    "    shape=X_train.shape[1:]\n",
    "\n",
    "    model = get_top_model_for_alexnet(\n",
    "        shape=shape,\n",
    "        nb_class=nb_class)\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "              nb_epoch=100, batch_size=32,verbose=1)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "    y_proba = model.predict(X_test)\n",
    "    y_pred = np_utils.probas_to_classes(y_proba)\n",
    "\n",
    "    target_names = ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "         'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    "         'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    "         'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    "         'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    "         'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    "         'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    "         'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    "         'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    "         'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    "         'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    "         'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    "         'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    "         'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    "         'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    "         'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    "         'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    "         'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng']\n",
    "    \n",
    "    print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "    print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "\n",
    "    model.save_weights(top_model_weights_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print \"Loading data..\"\n",
    "    data, labels, lz = load_data()\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    lz = np.array(lz)\n",
    "    print \"Data loaded !\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25)\n",
    "    print X_train.shape\n",
    "    print X_test.shape\n",
    "    print \"Test train splitted !\"\n",
    "\n",
    "    save_bottlebeck_features(X_train, X_test, y_train, y_test)\n",
    "    train_top_model(y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
