{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is disabled, cuDNN 5105)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "(20000,)\n",
      "Data loaded !\n",
      "Test train Shape: \n",
      "(10000, 3, 227, 227)\n",
      "(10000, 3, 227, 227)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/machine/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode='max')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Fold 1 / 2\n",
      "deep features extracted (256, 6, 6)\n",
      "\n",
      "Training CNN..\n",
      "Epoch 1/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.8549 - acc: 0.0105     \n",
      "Epoch 2/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.6489 - acc: 0.0181     \n",
      "Epoch 3/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.5331 - acc: 0.0296     \n",
      "Epoch 4/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.4559 - acc: 0.0382     \n",
      "Epoch 5/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.3777 - acc: 0.0488     \n",
      "Epoch 6/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.3192 - acc: 0.0561     \n",
      "Epoch 7/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.2586 - acc: 0.0663     \n",
      "Epoch 8/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.1941 - acc: 0.0791     \n",
      "Epoch 9/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.1302 - acc: 0.0867     \n",
      "Epoch 10/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.0639 - acc: 0.0931     \n",
      "Epoch 11/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.0050 - acc: 0.1020     \n",
      "Epoch 12/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.9488 - acc: 0.1162     \n",
      "Epoch 13/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.8925 - acc: 0.1207     \n",
      "Epoch 14/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.8337 - acc: 0.1287     \n",
      "Epoch 15/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.7815 - acc: 0.1381     \n",
      "Epoch 16/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.7249 - acc: 0.1504     \n",
      "Epoch 17/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.6751 - acc: 0.1504     \n",
      "Epoch 18/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.6411 - acc: 0.1596     \n",
      "Epoch 19/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.5791 - acc: 0.1700     \n",
      "Epoch 20/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.5289 - acc: 0.1743     \n",
      "Epoch 21/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.4988 - acc: 0.1794     \n",
      "Epoch 22/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.4452 - acc: 0.1911     \n",
      "Epoch 23/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.4163 - acc: 0.1962     \n",
      "Epoch 24/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.3672 - acc: 0.2079     \n",
      "Epoch 25/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.3256 - acc: 0.2116     \n",
      "Epoch 26/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.2865 - acc: 0.2125     \n",
      "Epoch 27/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.2542 - acc: 0.2210     \n",
      "Epoch 28/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.2139 - acc: 0.2316     \n",
      "Epoch 29/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.1732 - acc: 0.2453     \n",
      "Epoch 30/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.1358 - acc: 0.2470     \n",
      "Epoch 31/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.1116 - acc: 0.2491     \n",
      "Epoch 32/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.0611 - acc: 0.2593     \n",
      "Epoch 33/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.0290 - acc: 0.2702     \n",
      "Epoch 34/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.0068 - acc: 0.2754     \n",
      "Epoch 35/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.9673 - acc: 0.2808     \n",
      "Epoch 36/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.9315 - acc: 0.2827     \n",
      "Epoch 37/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.8999 - acc: 0.2912     \n",
      "Epoch 38/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.8712 - acc: 0.2992     \n",
      "Epoch 39/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.8577 - acc: 0.3020     \n",
      "Epoch 40/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.8107 - acc: 0.3089     \n",
      "Epoch 41/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.7732 - acc: 0.3164     \n",
      "Epoch 42/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.7619 - acc: 0.3223     \n",
      "Epoch 43/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.7334 - acc: 0.3196     \n",
      "Epoch 44/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.6919 - acc: 0.3394     \n",
      "Epoch 45/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.6716 - acc: 0.3347     \n",
      "Epoch 46/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.6428 - acc: 0.3481     \n",
      "Epoch 47/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.6100 - acc: 0.3570     \n",
      "Epoch 48/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.5811 - acc: 0.3589     \n",
      "Epoch 49/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.5574 - acc: 0.3668     \n",
      "Epoch 50/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.5359 - acc: 0.3719     \n",
      "Epoch 51/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.4974 - acc: 0.3833     \n",
      "Epoch 52/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.4845 - acc: 0.3766     \n",
      "Epoch 53/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.4566 - acc: 0.3855     \n",
      "Epoch 54/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.4385 - acc: 0.3935     \n",
      "Epoch 55/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.4187 - acc: 0.3949     \n",
      "Epoch 56/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.3876 - acc: 0.4019     \n",
      "Epoch 57/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.3625 - acc: 0.4059     \n",
      "Epoch 58/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.3114 - acc: 0.4178     \n",
      "Epoch 59/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.2979 - acc: 0.4194     \n",
      "Epoch 60/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.2873 - acc: 0.4243     \n",
      "Epoch 61/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.2636 - acc: 0.4275     \n",
      "Epoch 62/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.2452 - acc: 0.4320     \n",
      "Epoch 63/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.2221 - acc: 0.4401     \n",
      "Epoch 64/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.1981 - acc: 0.4438     \n",
      "Epoch 65/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.1792 - acc: 0.4457     \n",
      "Epoch 66/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.1398 - acc: 0.4606     \n",
      "Epoch 67/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.1302 - acc: 0.4588     \n",
      "Epoch 68/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.1184 - acc: 0.4585     \n",
      "Epoch 69/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.0803 - acc: 0.4746     \n",
      "Epoch 70/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.0715 - acc: 0.4731     \n",
      "Epoch 71/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.0567 - acc: 0.4825     \n",
      "Epoch 72/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.0152 - acc: 0.4875     \n",
      "Epoch 73/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.0130 - acc: 0.4898     \n",
      "Epoch 74/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.9800 - acc: 0.4986     \n",
      "Epoch 75/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.9676 - acc: 0.4971     \n",
      "Epoch 76/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.9459 - acc: 0.5064     \n",
      "Epoch 77/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.9275 - acc: 0.5083     \n",
      "Epoch 78/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.9147 - acc: 0.5119     \n",
      "Epoch 79/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.8910 - acc: 0.5246     \n",
      "Epoch 80/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.8804 - acc: 0.5218     \n",
      "Epoch 81/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.8478 - acc: 0.5286     \n",
      "Epoch 82/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.8303 - acc: 0.5338     \n",
      "Epoch 83/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7956 - acc: 0.5327     \n",
      "Epoch 84/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7965 - acc: 0.5408     \n",
      "Epoch 85/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7788 - acc: 0.5454     \n",
      "Epoch 86/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7563 - acc: 0.5498     \n",
      "Epoch 87/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7381 - acc: 0.5557     \n",
      "Epoch 88/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7261 - acc: 0.5626     \n",
      "Epoch 89/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7066 - acc: 0.5666     \n",
      "Epoch 90/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6893 - acc: 0.5640     \n",
      "Epoch 91/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6641 - acc: 0.5795     \n",
      "Epoch 92/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6483 - acc: 0.5812     \n",
      "Epoch 93/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6313 - acc: 0.5795     \n",
      "Epoch 94/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6218 - acc: 0.5878     \n",
      "Epoch 95/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5998 - acc: 0.5930     \n",
      "Epoch 96/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5795 - acc: 0.5946     \n",
      "Epoch 97/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5795 - acc: 0.5950     \n",
      "Epoch 98/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5524 - acc: 0.6014     \n",
      "Epoch 99/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5436 - acc: 0.6060     \n",
      "Epoch 100/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5386 - acc: 0.6076     \n",
      "Epoch 101/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5072 - acc: 0.6114     \n",
      "Epoch 102/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4986 - acc: 0.6154     \n",
      "Epoch 103/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4678 - acc: 0.6271     \n",
      "Epoch 104/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4428 - acc: 0.6315     \n",
      "Epoch 105/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4529 - acc: 0.6312     \n",
      "Epoch 106/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4106 - acc: 0.6385     \n",
      "Epoch 107/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4190 - acc: 0.6361     \n",
      "Epoch 108/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4011 - acc: 0.6472     \n",
      "Epoch 109/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3834 - acc: 0.6486     \n",
      "Epoch 110/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3657 - acc: 0.6521     \n",
      "Epoch 111/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3590 - acc: 0.6541     \n",
      "Epoch 112/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3551 - acc: 0.6504     \n",
      "Epoch 113/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3313 - acc: 0.6603     \n",
      "Epoch 114/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3133 - acc: 0.6675     \n",
      "Epoch 115/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3044 - acc: 0.6662     \n",
      "Epoch 116/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3007 - acc: 0.6665     \n",
      "Epoch 117/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2700 - acc: 0.6775     \n",
      "Epoch 118/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2664 - acc: 0.6792     \n",
      "Epoch 119/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2494 - acc: 0.6835     \n",
      "Epoch 120/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2391 - acc: 0.6865     \n",
      "Epoch 121/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2366 - acc: 0.6861     \n",
      "Epoch 122/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2235 - acc: 0.6910     \n",
      "Epoch 123/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2050 - acc: 0.6940     \n",
      "Epoch 124/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1987 - acc: 0.7013     \n",
      "Epoch 125/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1802 - acc: 0.7037     \n",
      "Epoch 126/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1601 - acc: 0.7064     \n",
      "Epoch 127/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1559 - acc: 0.7079     \n",
      "Epoch 128/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1342 - acc: 0.7177     \n",
      "Epoch 129/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1365 - acc: 0.7131     \n",
      "Epoch 130/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1046 - acc: 0.7226     \n",
      "Epoch 131/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1059 - acc: 0.7225     \n",
      "Epoch 132/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0786 - acc: 0.7332     \n",
      "Epoch 133/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0804 - acc: 0.7290     \n",
      "Epoch 134/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0720 - acc: 0.7347     \n",
      "Epoch 135/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0596 - acc: 0.7354     \n",
      "Epoch 136/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0502 - acc: 0.7420     \n",
      "Epoch 137/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0420 - acc: 0.7429     \n",
      "Epoch 138/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0243 - acc: 0.7517     \n",
      "Epoch 139/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0113 - acc: 0.7543     \n",
      "Epoch 140/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0172 - acc: 0.7454     \n",
      "Epoch 141/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9960 - acc: 0.7563     \n",
      "Epoch 142/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9873 - acc: 0.7544     \n",
      "Epoch 143/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9736 - acc: 0.7632     \n",
      "Epoch 144/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9679 - acc: 0.7619     \n",
      "Epoch 145/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9684 - acc: 0.7615     \n",
      "Epoch 146/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9502 - acc: 0.7642     \n",
      "Epoch 147/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9376 - acc: 0.7714     \n",
      "Epoch 148/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9287 - acc: 0.7719     \n",
      "Epoch 149/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9110 - acc: 0.7770     \n",
      "Epoch 150/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9112 - acc: 0.7799     \n",
      "CNN acc: 93.07%\n",
      "Weights for \"dense_1\" are loaded\n",
      "Weights for \"dense_2\" are loaded\n",
      "Weights for \"dense_3\" are loaded\n",
      "deep features extracted (4096,)\n",
      "\n",
      "Training SVM..\n",
      "SVM acc: : 51.98%\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "           AisKacang       0.54      0.62      0.58       100\n",
      "           AngKuKueh       0.35      0.44      0.39       100\n",
      "           ApamBalik       0.56      0.63      0.59       100\n",
      "           Asamlaksa       0.42      0.48      0.45       100\n",
      "              Bahulu       0.78      0.89      0.83       100\n",
      "           Bakkukteh       0.63      0.63      0.63       100\n",
      "      BananaLeafRice       0.68      0.58      0.63       100\n",
      "             Bazhang       0.48      0.50      0.49       100\n",
      "         BeefRendang       0.76      0.90      0.83       100\n",
      "           BingkaUbi       0.59      0.75      0.66       100\n",
      "         Buburchacha       0.45      0.58      0.50       100\n",
      "          Buburpedas       0.61      0.73      0.67       100\n",
      "              Capati       0.42      0.49      0.45       100\n",
      "              Cendol       0.42      0.56      0.48       100\n",
      "         ChaiTowKuay       0.70      0.84      0.76       100\n",
      "        CharKuehTiao       0.64      0.73      0.68       100\n",
      "             CharSiu       0.76      0.75      0.75       100\n",
      "       CheeCheongFun       0.70      0.93      0.80       100\n",
      "           ChiliCrab       0.36      0.32      0.34       100\n",
      "           Chweekueh       0.62      0.76      0.68       100\n",
      "         ClayPotRice       0.39      0.32      0.35       100\n",
      "          CucurUdang       0.44      0.51      0.47       100\n",
      "          CurryLaksa       0.29      0.23      0.26       100\n",
      "           CurryPuff       0.33      0.37      0.35       100\n",
      "               Dodol       0.41      0.54      0.47       100\n",
      "              Durian       0.42      0.47      0.45       100\n",
      "         DurianCrepe       0.38      0.39      0.38       100\n",
      "       FishHeadCurry       0.27      0.25      0.26       100\n",
      "               Guava       0.36      0.43      0.39       100\n",
      "HainaneseChickenRice       0.38      0.30      0.33       100\n",
      "          HokkienMee       0.48      0.47      0.47       100\n",
      "            Huatkuih       0.73      0.74      0.74       100\n",
      "           IkanBakar       0.62      0.65      0.63       100\n",
      "            Kangkung       0.58      0.56      0.57       100\n",
      "           KayaToast       0.39      0.37      0.38       100\n",
      "            Keklapis       0.32      0.40      0.36       100\n",
      "             Ketupat       0.56      0.62      0.59       100\n",
      "           KuihDadar       0.26      0.23      0.25       100\n",
      "           KuihLapis       0.67      0.78      0.72       100\n",
      "        KuihSeriMuka       0.45      0.45      0.45       100\n",
      "             Langsat       0.51      0.53      0.52       100\n",
      "               Lekor       0.37      0.44      0.40       100\n",
      "              Lemang       0.71      0.64      0.67       100\n",
      "         LepatPisang       0.41      0.41      0.41       100\n",
      "              LorMee       0.55      0.68      0.61       100\n",
      "        Maggi goreng       0.79      0.79      0.79       100\n",
      "          Mangosteen       0.55      0.45      0.49       100\n",
      "           MeeGoreng       0.43      0.43      0.43       100\n",
      "         MeeHoonKueh       0.51      0.52      0.51       100\n",
      "         MeeHoonSoup       0.30      0.33      0.31       100\n",
      "             MeeJawa       0.66      0.73      0.69       100\n",
      "            MeeRebus       0.58      0.52      0.55       100\n",
      "            MeeRojak       0.43      0.35      0.38       100\n",
      "             MeeSiam       0.50      0.57      0.53       100\n",
      "            Murtabak       0.54      0.45      0.49       100\n",
      "             Murukku       0.54      0.57      0.56       100\n",
      "   NasiGorengKampung       0.64      0.61      0.62       100\n",
      "           NasiImpit       0.47      0.61      0.53       100\n",
      "          Nasikandar       0.63      0.71      0.67       100\n",
      "           Nasilemak       0.28      0.24      0.26       100\n",
      "         Nasipattaya       0.66      0.64      0.65       100\n",
      "          Ondehondeh       0.39      0.38      0.39       100\n",
      "            Otakotak       0.63      0.67      0.65       100\n",
      "      OysterOmelette       0.47      0.45      0.46       100\n",
      "              PanMee       0.40      0.42      0.41       100\n",
      "       PineappleTart       0.26      0.24      0.25       100\n",
      "        PisangGoreng       0.67      0.58      0.62       100\n",
      "              Popiah       0.59      0.68      0.63       100\n",
      "            PrawnMee       0.38      0.40      0.39       100\n",
      "         Prawnsambal       0.56      0.49      0.52       100\n",
      "                Puri       0.69      0.47      0.56       100\n",
      "           PutuMayam       0.48      0.51      0.49       100\n",
      "          PutuPiring       0.28      0.17      0.21       100\n",
      "            Rambutan       0.49      0.37      0.42       100\n",
      "               Rojak       0.78      0.95      0.86       100\n",
      "           RotiCanai       0.47      0.40      0.43       100\n",
      "            RotiJala       0.56      0.49      0.52       100\n",
      "            RotiJohn       0.63      0.77      0.69       100\n",
      "            RotiNaan       0.45      0.53      0.49       100\n",
      "          RotiTissue       0.31      0.19      0.24       100\n",
      "         SambalPetai       0.56      0.45      0.50       100\n",
      "         SambalUdang       0.57      0.43      0.49       100\n",
      "               Satay       0.53      0.53      0.53       100\n",
      "          Sataycelup       0.56      0.51      0.53       100\n",
      "            SeriMuka       0.84      0.87      0.86       100\n",
      "            SotoAyam       0.64      0.60      0.62       100\n",
      "     TandooriChicken       0.36      0.27      0.31       100\n",
      "            TangYuan       0.60      0.44      0.51       100\n",
      "           TauFooFah       0.66      0.74      0.70       100\n",
      "         TauhuSumbat       0.32      0.24      0.27       100\n",
      "              Thosai       0.43      0.43      0.43       100\n",
      "          TomYumSoup       0.80      0.86      0.83       100\n",
      "               Wajik       0.29      0.16      0.21       100\n",
      "           WanTanMee       0.45      0.34      0.39       100\n",
      "             WaTanHo       0.42      0.36      0.39       100\n",
      "              Wonton       0.45      0.27      0.34       100\n",
      "             YamCake       0.40      0.31      0.35       100\n",
      "           YongTauFu       0.78      0.74      0.76       100\n",
      "             Youtiao       0.43      0.35      0.39       100\n",
      "             Yusheng       0.60      0.51      0.55       100\n",
      "\n",
      "         avg / total       0.51      0.52      0.51     10000\n",
      "\n",
      "[[62  0  2 ...,  0  0  0]\n",
      " [ 0 44  0 ...,  0  0  0]\n",
      " [ 1  0 63 ...,  1  0  0]\n",
      " ..., \n",
      " [ 0  0  0 ..., 74  0  0]\n",
      " [ 0  0  1 ...,  1 35  0]\n",
      " [ 4  0  0 ...,  0  0 51]]\n",
      "Test train Shape: \n",
      "(10000, 3, 227, 227)\n",
      "(10000, 3, 227, 227)\n",
      "Running Fold 2 / 2\n",
      "deep features extracted (256, 6, 6)\n",
      "\n",
      "Training CNN..\n",
      "Epoch 1/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.8470 - acc: 0.0125     \n",
      "Epoch 2/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.6370 - acc: 0.0192     \n",
      "Epoch 3/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.5371 - acc: 0.0281     \n",
      "Epoch 4/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.4600 - acc: 0.0369     \n",
      "Epoch 5/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.3875 - acc: 0.0485     \n",
      "Epoch 6/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.3275 - acc: 0.0555     \n",
      "Epoch 7/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.2567 - acc: 0.0646     \n",
      "Epoch 8/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.2028 - acc: 0.0733     \n",
      "Epoch 9/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.1400 - acc: 0.0803     \n",
      "Epoch 10/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.0701 - acc: 0.0915     \n",
      "Epoch 11/150\n",
      "10000/10000 [==============================] - 8s - loss: 4.0049 - acc: 0.1014     \n",
      "Epoch 12/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.9590 - acc: 0.1078     \n",
      "Epoch 13/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.8946 - acc: 0.1136     \n",
      "Epoch 14/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.8505 - acc: 0.1235     \n",
      "Epoch 15/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.7895 - acc: 0.1360     \n",
      "Epoch 16/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.7251 - acc: 0.1416     \n",
      "Epoch 17/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.6868 - acc: 0.1517     \n",
      "Epoch 18/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.6476 - acc: 0.1537     \n",
      "Epoch 19/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.5905 - acc: 0.1665     \n",
      "Epoch 20/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.5571 - acc: 0.1742     \n",
      "Epoch 21/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.5085 - acc: 0.1788     \n",
      "Epoch 22/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.4634 - acc: 0.1883     \n",
      "Epoch 23/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.4247 - acc: 0.1904     \n",
      "Epoch 24/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.3828 - acc: 0.2021     \n",
      "Epoch 25/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.3394 - acc: 0.2086     \n",
      "Epoch 26/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.3124 - acc: 0.2164     \n",
      "Epoch 27/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.2769 - acc: 0.2226     \n",
      "Epoch 28/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.2285 - acc: 0.2311     \n",
      "Epoch 29/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.1986 - acc: 0.2367     \n",
      "Epoch 30/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.1481 - acc: 0.2438     \n",
      "Epoch 31/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.1233 - acc: 0.2475     \n",
      "Epoch 32/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.0885 - acc: 0.2565     \n",
      "Epoch 33/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.0490 - acc: 0.2593     \n",
      "Epoch 34/150\n",
      "10000/10000 [==============================] - 8s - loss: 3.0297 - acc: 0.2664     \n",
      "Epoch 35/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.9919 - acc: 0.2747     \n",
      "Epoch 36/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.9543 - acc: 0.2816     \n",
      "Epoch 37/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.9305 - acc: 0.2851     \n",
      "Epoch 38/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.8999 - acc: 0.2924     \n",
      "Epoch 39/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.8597 - acc: 0.2968     \n",
      "Epoch 40/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.8327 - acc: 0.3067     \n",
      "Epoch 41/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.7909 - acc: 0.3128     \n",
      "Epoch 42/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.7803 - acc: 0.3247     \n",
      "Epoch 43/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.7508 - acc: 0.3272     \n",
      "Epoch 44/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.7192 - acc: 0.3337     \n",
      "Epoch 45/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.6858 - acc: 0.3392     \n",
      "Epoch 46/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.6566 - acc: 0.3471     \n",
      "Epoch 47/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.6396 - acc: 0.3518     \n",
      "Epoch 48/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.6039 - acc: 0.3555     \n",
      "Epoch 49/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.5707 - acc: 0.3653     \n",
      "Epoch 50/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.5629 - acc: 0.3559     \n",
      "Epoch 51/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.5146 - acc: 0.3777     \n",
      "Epoch 52/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.5102 - acc: 0.3760     \n",
      "Epoch 53/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.4741 - acc: 0.3816     \n",
      "Epoch 54/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.4695 - acc: 0.3780     \n",
      "Epoch 55/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.4361 - acc: 0.3938     \n",
      "Epoch 56/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.4001 - acc: 0.4040     \n",
      "Epoch 57/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.3665 - acc: 0.4090     \n",
      "Epoch 58/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.3526 - acc: 0.4128     \n",
      "Epoch 59/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.3330 - acc: 0.4181     \n",
      "Epoch 60/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.3134 - acc: 0.4192     \n",
      "Epoch 61/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.2810 - acc: 0.4258     \n",
      "Epoch 62/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.2565 - acc: 0.4321     \n",
      "Epoch 63/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.2360 - acc: 0.4368     \n",
      "Epoch 64/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.2196 - acc: 0.4384     \n",
      "Epoch 65/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.1963 - acc: 0.4452     \n",
      "Epoch 66/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.1734 - acc: 0.4510     \n",
      "Epoch 67/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.1479 - acc: 0.4608     \n",
      "Epoch 68/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.1251 - acc: 0.4621     \n",
      "Epoch 69/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.1152 - acc: 0.4602     \n",
      "Epoch 70/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.0927 - acc: 0.4692     \n",
      "Epoch 71/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.0666 - acc: 0.4786     \n",
      "Epoch 72/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.0570 - acc: 0.4733     \n",
      "Epoch 73/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.0316 - acc: 0.4831     \n",
      "Epoch 74/150\n",
      "10000/10000 [==============================] - 8s - loss: 2.0186 - acc: 0.4855     \n",
      "Epoch 75/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.9922 - acc: 0.4972     \n",
      "Epoch 76/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.9701 - acc: 0.5027     \n",
      "Epoch 77/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.9482 - acc: 0.5057     \n",
      "Epoch 78/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.9311 - acc: 0.5085     \n",
      "Epoch 79/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.9083 - acc: 0.5179     \n",
      "Epoch 80/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.8993 - acc: 0.5186     \n",
      "Epoch 81/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.8680 - acc: 0.5183     \n",
      "Epoch 82/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.8464 - acc: 0.5355     \n",
      "Epoch 83/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.8359 - acc: 0.5274     \n",
      "Epoch 84/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.8224 - acc: 0.5341     \n",
      "Epoch 85/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.8028 - acc: 0.5393     \n",
      "Epoch 86/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7965 - acc: 0.5396     \n",
      "Epoch 87/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7562 - acc: 0.5493     \n",
      "Epoch 88/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7497 - acc: 0.5564     \n",
      "Epoch 89/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7331 - acc: 0.5543     \n",
      "Epoch 90/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.7199 - acc: 0.5606     \n",
      "Epoch 91/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6990 - acc: 0.5667     \n",
      "Epoch 92/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6791 - acc: 0.5762     \n",
      "Epoch 93/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6666 - acc: 0.5787     \n",
      "Epoch 94/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6509 - acc: 0.5767     \n",
      "Epoch 95/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6290 - acc: 0.5844     \n",
      "Epoch 96/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6142 - acc: 0.5847     \n",
      "Epoch 97/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.6054 - acc: 0.5936     \n",
      "Epoch 98/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5877 - acc: 0.5943     \n",
      "Epoch 99/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5748 - acc: 0.5949     \n",
      "Epoch 100/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5472 - acc: 0.6083     \n",
      "Epoch 101/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5396 - acc: 0.6043     \n",
      "Epoch 102/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.5215 - acc: 0.6148     \n",
      "Epoch 103/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4949 - acc: 0.6178     \n",
      "Epoch 104/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4940 - acc: 0.6132     \n",
      "Epoch 105/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4629 - acc: 0.6289     \n",
      "Epoch 106/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4606 - acc: 0.6307     \n",
      "Epoch 107/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4482 - acc: 0.6273     \n",
      "Epoch 108/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4237 - acc: 0.6384     \n",
      "Epoch 109/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.4148 - acc: 0.6402     \n",
      "Epoch 110/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3929 - acc: 0.6487     \n",
      "Epoch 111/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3775 - acc: 0.6538     \n",
      "Epoch 112/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3591 - acc: 0.6572     \n",
      "Epoch 113/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3553 - acc: 0.6577     \n",
      "Epoch 114/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3440 - acc: 0.6572     \n",
      "Epoch 115/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3334 - acc: 0.6600     \n",
      "Epoch 116/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3110 - acc: 0.6667     \n",
      "Epoch 117/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.3008 - acc: 0.6714     \n",
      "Epoch 118/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2840 - acc: 0.6743     \n",
      "Epoch 119/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2764 - acc: 0.6778     \n",
      "Epoch 120/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2654 - acc: 0.6823     \n",
      "Epoch 121/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2527 - acc: 0.6864     \n",
      "Epoch 122/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2414 - acc: 0.6863     \n",
      "Epoch 123/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2229 - acc: 0.6956     \n",
      "Epoch 124/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2096 - acc: 0.6958     \n",
      "Epoch 125/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.2011 - acc: 0.6997     \n",
      "Epoch 126/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1933 - acc: 0.7005     \n",
      "Epoch 127/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1763 - acc: 0.7047     \n",
      "Epoch 128/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1595 - acc: 0.7130     \n",
      "Epoch 129/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1576 - acc: 0.7090     \n",
      "Epoch 130/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1497 - acc: 0.7136     \n",
      "Epoch 131/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1247 - acc: 0.7182     \n",
      "Epoch 132/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1119 - acc: 0.7260     \n",
      "Epoch 133/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1056 - acc: 0.7252     \n",
      "Epoch 134/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.1040 - acc: 0.7229     \n",
      "Epoch 135/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0836 - acc: 0.7302     \n",
      "Epoch 136/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0794 - acc: 0.7335     \n",
      "Epoch 137/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0523 - acc: 0.7394     \n",
      "Epoch 138/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0425 - acc: 0.7440     \n",
      "Epoch 139/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0433 - acc: 0.7385     \n",
      "Epoch 140/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0299 - acc: 0.7474     \n",
      "Epoch 141/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0238 - acc: 0.7479     \n",
      "Epoch 142/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0122 - acc: 0.7500     \n",
      "Epoch 143/150\n",
      "10000/10000 [==============================] - 8s - loss: 1.0020 - acc: 0.7536     \n",
      "Epoch 144/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9858 - acc: 0.7589     \n",
      "Epoch 145/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9808 - acc: 0.7564     \n",
      "Epoch 146/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9568 - acc: 0.7649     \n",
      "Epoch 147/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9641 - acc: 0.7631     \n",
      "Epoch 148/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9550 - acc: 0.7650     \n",
      "Epoch 149/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9399 - acc: 0.7699     \n",
      "Epoch 150/150\n",
      "10000/10000 [==============================] - 8s - loss: 0.9231 - acc: 0.7768     \n",
      "CNN acc: 91.99%\n",
      "Weights for \"dense_1\" are loaded\n",
      "Weights for \"dense_2\" are loaded\n",
      "Weights for \"dense_3\" are loaded\n",
      "deep features extracted (4096,)\n",
      "\n",
      "Training SVM..\n",
      "SVM acc: : 52.56%\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "           AisKacang       0.63      0.64      0.64       100\n",
      "           AngKuKueh       0.30      0.40      0.34       100\n",
      "           ApamBalik       0.45      0.49      0.47       100\n",
      "           Asamlaksa       0.39      0.42      0.40       100\n",
      "              Bahulu       0.67      0.97      0.80       100\n",
      "           Bakkukteh       0.61      0.74      0.67       100\n",
      "      BananaLeafRice       0.45      0.48      0.46       100\n",
      "             Bazhang       0.48      0.58      0.52       100\n",
      "         BeefRendang       0.74      0.93      0.82       100\n",
      "           BingkaUbi       0.63      0.83      0.72       100\n",
      "         Buburchacha       0.51      0.53      0.52       100\n",
      "          Buburpedas       0.66      0.61      0.63       100\n",
      "              Capati       0.40      0.47      0.43       100\n",
      "              Cendol       0.38      0.50      0.43       100\n",
      "         ChaiTowKuay       0.71      0.79      0.75       100\n",
      "        CharKuehTiao       0.70      0.69      0.69       100\n",
      "             CharSiu       0.64      0.77      0.70       100\n",
      "       CheeCheongFun       0.72      0.80      0.76       100\n",
      "           ChiliCrab       0.38      0.38      0.38       100\n",
      "           Chweekueh       0.66      0.66      0.66       100\n",
      "         ClayPotRice       0.36      0.28      0.31       100\n",
      "          CucurUdang       0.47      0.35      0.40       100\n",
      "          CurryLaksa       0.36      0.27      0.31       100\n",
      "           CurryPuff       0.32      0.38      0.35       100\n",
      "               Dodol       0.42      0.47      0.45       100\n",
      "              Durian       0.51      0.48      0.49       100\n",
      "         DurianCrepe       0.40      0.49      0.44       100\n",
      "       FishHeadCurry       0.34      0.30      0.32       100\n",
      "               Guava       0.41      0.42      0.42       100\n",
      "HainaneseChickenRice       0.38      0.36      0.37       100\n",
      "          HokkienMee       0.49      0.49      0.49       100\n",
      "            Huatkuih       0.66      0.76      0.71       100\n",
      "           IkanBakar       0.55      0.66      0.60       100\n",
      "            Kangkung       0.52      0.50      0.51       100\n",
      "           KayaToast       0.42      0.54      0.47       100\n",
      "            Keklapis       0.33      0.30      0.31       100\n",
      "             Ketupat       0.59      0.63      0.61       100\n",
      "           KuihDadar       0.33      0.30      0.31       100\n",
      "           KuihLapis       0.66      0.83      0.74       100\n",
      "        KuihSeriMuka       0.48      0.33      0.39       100\n",
      "             Langsat       0.46      0.57      0.51       100\n",
      "               Lekor       0.37      0.41      0.39       100\n",
      "              Lemang       0.57      0.66      0.61       100\n",
      "         LepatPisang       0.44      0.44      0.44       100\n",
      "              LorMee       0.58      0.62      0.60       100\n",
      "        Maggi goreng       0.81      0.80      0.80       100\n",
      "          Mangosteen       0.47      0.39      0.43       100\n",
      "           MeeGoreng       0.53      0.49      0.51       100\n",
      "         MeeHoonKueh       0.50      0.54      0.52       100\n",
      "         MeeHoonSoup       0.45      0.42      0.44       100\n",
      "             MeeJawa       0.60      0.74      0.66       100\n",
      "            MeeRebus       0.60      0.57      0.58       100\n",
      "            MeeRojak       0.42      0.50      0.46       100\n",
      "             MeeSiam       0.48      0.50      0.49       100\n",
      "            Murtabak       0.53      0.55      0.54       100\n",
      "             Murukku       0.54      0.54      0.54       100\n",
      "   NasiGorengKampung       0.58      0.64      0.61       100\n",
      "           NasiImpit       0.54      0.52      0.53       100\n",
      "          Nasikandar       0.65      0.68      0.67       100\n",
      "           Nasilemak       0.30      0.23      0.26       100\n",
      "         Nasipattaya       0.63      0.67      0.65       100\n",
      "          Ondehondeh       0.53      0.34      0.41       100\n",
      "            Otakotak       0.63      0.78      0.70       100\n",
      "      OysterOmelette       0.46      0.45      0.46       100\n",
      "              PanMee       0.39      0.21      0.27       100\n",
      "       PineappleTart       0.39      0.35      0.37       100\n",
      "        PisangGoreng       0.60      0.64      0.62       100\n",
      "              Popiah       0.64      0.69      0.66       100\n",
      "            PrawnMee       0.46      0.42      0.44       100\n",
      "         Prawnsambal       0.63      0.54      0.58       100\n",
      "                Puri       0.64      0.47      0.54       100\n",
      "           PutuMayam       0.38      0.43      0.40       100\n",
      "          PutuPiring       0.29      0.23      0.26       100\n",
      "            Rambutan       0.47      0.30      0.37       100\n",
      "               Rojak       0.83      0.93      0.88       100\n",
      "           RotiCanai       0.45      0.34      0.39       100\n",
      "            RotiJala       0.64      0.56      0.60       100\n",
      "            RotiJohn       0.71      0.76      0.73       100\n",
      "            RotiNaan       0.58      0.51      0.54       100\n",
      "          RotiTissue       0.35      0.24      0.29       100\n",
      "         SambalPetai       0.58      0.55      0.56       100\n",
      "         SambalUdang       0.49      0.49      0.49       100\n",
      "               Satay       0.45      0.52      0.48       100\n",
      "          Sataycelup       0.52      0.58      0.55       100\n",
      "            SeriMuka       0.93      0.87      0.90       100\n",
      "            SotoAyam       0.65      0.62      0.64       100\n",
      "     TandooriChicken       0.43      0.35      0.38       100\n",
      "            TangYuan       0.64      0.56      0.60       100\n",
      "           TauFooFah       0.67      0.74      0.70       100\n",
      "         TauhuSumbat       0.29      0.21      0.24       100\n",
      "              Thosai       0.49      0.47      0.48       100\n",
      "          TomYumSoup       0.84      0.84      0.84       100\n",
      "               Wajik       0.34      0.25      0.29       100\n",
      "           WanTanMee       0.41      0.43      0.42       100\n",
      "             WaTanHo       0.47      0.47      0.47       100\n",
      "              Wonton       0.43      0.32      0.37       100\n",
      "             YamCake       0.28      0.20      0.23       100\n",
      "           YongTauFu       0.72      0.78      0.75       100\n",
      "             Youtiao       0.39      0.24      0.30       100\n",
      "             Yusheng       0.61      0.58      0.59       100\n",
      "\n",
      "         avg / total       0.52      0.53      0.52     10000\n",
      "\n",
      "[[64  0  0 ...,  0  0  2]\n",
      " [ 0 40  0 ...,  0  0  0]\n",
      " [ 0  2 49 ...,  0  1  0]\n",
      " ..., \n",
      " [ 0  0  0 ..., 78  0  0]\n",
      " [ 0  2  1 ...,  0 24  0]\n",
      " [ 2  0  0 ...,  0  0 58]]\n",
      "Average acc : 52.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode='max')\n",
      "/home/machine/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode='max')\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Flatten, Dense, Dropout, Reshape, Permute, Activation, \\\n",
    "    Input, merge\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from convnetskeras.customlayers import convolution2Dgroup, crosschannelnormalization, \\\n",
    "    splittensor, Softmax4D\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.utils import np_utils\n",
    "from sklearn import svm\n",
    "\n",
    "import sys, glob, argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "\n",
    "import hickle as hkl\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# path to the model weights file.\n",
    "weights_path = '../dataset/alexnet_weights.h5'\n",
    "nb_train_samples = 10000\n",
    "nb_validation_samples = 10000\n",
    "nb_class = 100\n",
    "nb_epoch = 150\n",
    "fold_count = 1\n",
    "\n",
    "def get_layer_weights(weights_file=None, layer_name=None):\n",
    "    if not weights_file or not layer_name:\n",
    "        return None\n",
    "    else:\n",
    "        g = weights_file[layer_name]\n",
    "        weights = [g[p] for p in g]\n",
    "        print 'Weights for \"{}\" are loaded'.format(layer_name)\n",
    "    return weights\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.jet):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(100)\n",
    "    plt.xticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "         'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    "         'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    "         'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    "         'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    "         'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    "         'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    "         'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    "         'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    "         'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    "         'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    "         'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    "         'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    "         'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    "         'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    "         'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    "         'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    "         'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "         'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    "         'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    "         'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    "         'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    "         'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    "         'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    "         'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    "         'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    "         'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    "         'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    "         'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    "         'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    "         'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    "         'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    "         'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    "         'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    "         'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def visualize_loss(hist):\n",
    "    train_loss=hist.history['loss']\n",
    "    val_loss=hist.history['val_loss']\n",
    "    train_acc=hist.history['acc']\n",
    "    val_acc=hist.history['val_acc']\n",
    "    xc=range(nb_epoch)\n",
    "\n",
    "    plt.figure(1,figsize=(7,5))\n",
    "    plt.plot(xc,train_loss)\n",
    "    plt.plot(xc,val_loss)\n",
    "    plt.xlabel('num of Epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('train_loss vs val_loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['train','val'])\n",
    "    #print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "    plt.style.use(['classic'])\n",
    "\n",
    "    plt.figure(2,figsize=(7,5))\n",
    "    plt.plot(xc,train_acc)\n",
    "    plt.plot(xc,val_acc)\n",
    "    plt.xlabel('num of Epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('train_acc vs val_acc')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['train','val'],loc=4)\n",
    "    #print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "    plt.style.use(['classic'])\n",
    "    plt.show()\n",
    "\n",
    "def load_data():\n",
    "    # load your data using this function\n",
    "    d = hkl.load('../dataset/myfood100-227.hkl')\n",
    "    data = d['trainFeatures']\n",
    "    labels = d['trainLabels']\n",
    "    lz = d['labels']\n",
    "    data = data.reshape(data.shape[0], 3, 227, 227)\n",
    "    #data = data.transpose(0, 2, 3, 1)\n",
    "\n",
    "    return data,labels,lz\n",
    "\n",
    "def get_top_model_for_alexnet(nb_class=None, shape=None, W_regularizer=False, weights_file_path=None, input=None, output=None):\n",
    "    if not output:\n",
    "        inputs = Input(shape=shape)\n",
    "    \n",
    "    dense_1 = Flatten(name=\"flatten\")(inputs)\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1')(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2')(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(nb_class,name='dense_3')(dense_3)\n",
    "    predictions = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "    model = Model(input=input or inputs, output=predictions)\n",
    "\n",
    "    return model\n",
    "\n",
    "# This function load top model pretrained weights, will be merge with alexnet bottom model weight\n",
    "def get_top_model_for_svm(nb_class=None, shape=None, W_regularizer=False, weights_file_path=None, input=None, output=None):\n",
    "    if not output:\n",
    "        inputs = Input(shape=shape)\n",
    "        dense_1 = Flatten(name='flatten')(inputs)\n",
    "    else:\n",
    "        dense_1 = Flatten(name='flatten', input_shape=shape)(output)\n",
    "\n",
    "    if weights_file_path:\n",
    "        weights_file = h5.File(weights_file_path)\n",
    "\n",
    "    weights_1 = get_layer_weights(weights_file, 'dense_1')\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1',weights=weights_1)(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "\n",
    "    weights_2 = get_layer_weights(weights_file, 'dense_2')\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2',weights=weights_2)(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "\n",
    "    weights_3 = get_layer_weights(weights_file, 'dense_3')\n",
    "    dense_3 = Dense(nb_class,name='dense_3',weights=weights_3)(dense_3)\n",
    "    predictions = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "    model = Model(input=input or inputs, output=dense_2)\n",
    "\n",
    "\n",
    "    if weights_file:\n",
    "        weights_file.close()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Model for extracting bottleneck \n",
    "def load_model(nb_class, weights_path=None):\n",
    "\n",
    "    inputs = Input(shape=(3,227,227))\n",
    "\n",
    "    conv_1 = Convolution2D(96, 11, 11,subsample=(4,4),activation='relu',\n",
    "                           name='conv_1')(inputs)\n",
    "\n",
    "    conv_2 = MaxPooling2D((3, 3), strides=(2,2))(conv_1)\n",
    "    conv_2 = crosschannelnormalization(name=\"convpool_1\")(conv_2)\n",
    "    conv_2 = ZeroPadding2D((2,2))(conv_2)\n",
    "    conv_2 = merge([\n",
    "        Convolution2D(128,5,5,activation=\"relu\",name='conv_2_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_2)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_2\")\n",
    "\n",
    "    conv_3 = MaxPooling2D((3, 3), strides=(2, 2))(conv_2)\n",
    "    conv_3 = crosschannelnormalization()(conv_3)\n",
    "    conv_3 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_3 = Convolution2D(384,3,3,activation='relu',name='conv_3')(conv_3)\n",
    "\n",
    "    conv_4 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_4 = merge([\n",
    "        Convolution2D(192,3,3,activation=\"relu\",name='conv_4_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_4)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_4\")\n",
    "\n",
    "    conv_5 = ZeroPadding2D((1,1))(conv_4)\n",
    "    conv_5 = merge([\n",
    "        Convolution2D(128,3,3,activation=\"relu\",name='conv_5_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_5)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_5\")\n",
    "\n",
    "    conv_5 = MaxPooling2D((3, 3), strides=(2,2),name=\"convpool_5\")(conv_5)\n",
    "\n",
    "    dense_1 = Flatten(name=\"flatten\")(conv_5)\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1')(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2')(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(nb_class,name='dense_3')(dense_3)\n",
    "    prediction = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "\n",
    "\n",
    "    base_model = Model(input=inputs, output=prediction)\n",
    "\n",
    "    if weights_path:\n",
    "        base_model.load_weights(weights_path)\n",
    "\n",
    "    base_model = Model(input=inputs, output=conv_5)\n",
    "\n",
    "    return base_model\n",
    "\n",
    "# Model for merging bottom alexnet model weights and finetuned top model weights \n",
    "def load_svm_model(nb_class, weights_path=None):\n",
    "\n",
    "    inputs = Input(shape=(3,227,227))\n",
    "\n",
    "    conv_1 = Convolution2D(96, 11, 11,subsample=(4,4),activation='relu',\n",
    "                           name='conv_1')(inputs)\n",
    "\n",
    "    conv_2 = MaxPooling2D((3, 3), strides=(2,2))(conv_1)\n",
    "    conv_2 = crosschannelnormalization(name=\"convpool_1\")(conv_2)\n",
    "    conv_2 = ZeroPadding2D((2,2))(conv_2)\n",
    "    conv_2 = merge([\n",
    "        Convolution2D(128,5,5,activation=\"relu\",name='conv_2_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_2)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_2\")\n",
    "\n",
    "    conv_3 = MaxPooling2D((3, 3), strides=(2, 2))(conv_2)\n",
    "    conv_3 = crosschannelnormalization()(conv_3)\n",
    "    conv_3 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_3 = Convolution2D(384,3,3,activation='relu',name='conv_3')(conv_3)\n",
    "\n",
    "    conv_4 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_4 = merge([\n",
    "        Convolution2D(192,3,3,activation=\"relu\",name='conv_4_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_4)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_4\")\n",
    "\n",
    "    conv_5 = ZeroPadding2D((1,1))(conv_4)\n",
    "    conv_5 = merge([\n",
    "        Convolution2D(128,3,3,activation=\"relu\",name='conv_5_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_5)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_5\")\n",
    "\n",
    "    conv_5 = MaxPooling2D((3, 3), strides=(2,2),name=\"convpool_5\")(conv_5)\n",
    "\n",
    "    dense_1 = Flatten(name=\"flatten\")(conv_5)\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1')(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2')(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(nb_class,name='dense_3')(dense_3)\n",
    "    prediction = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "\n",
    "\n",
    "    base_model = Model(input=inputs, output=prediction)\n",
    "\n",
    "    if weights_path:\n",
    "        base_model.load_weights(weights_path)\n",
    "\n",
    "\n",
    "    #model = Model(input=inputs, output=conv_5)\n",
    "\n",
    "    base_model = Model(input=inputs, output=conv_5)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = get_top_model_for_svm(\n",
    "        shape=base_model.output_shape[1:],\n",
    "        nb_class=nb_class,\n",
    "        weights_file_path=\"model/alex_topmodel\" + str(fold_count) + \".h5\",\n",
    "        input= base_model.input,\n",
    "        output= base_model.output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Save CNN bottleneck for finetune top model \n",
    "def save_bottleneck_features(X_train, X_test, y_train, y_test):\n",
    "    model = load_model(nb_class=nb_class, weights_path=weights_path)\n",
    "\n",
    "    bottleneck_features_train = model.predict(X_train)\n",
    "    np.save(open('alex_bottleneck_features_train'+ str(fold_count) +'.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "\n",
    "    bottleneck_features_validation = model.predict(X_test)\n",
    "    np.save(open('alex_bottleneck_features_validation'+ str(fold_count) + '.npy', 'wb'), bottleneck_features_validation)\n",
    "    print \"deep features extracted\", bottleneck_features_train.shape[1:]\n",
    "\n",
    "# Save finetuned CNN for svm classification\n",
    "def save_bottleneck_svmfeatures(X_train, X_test, y_train, y_test):\n",
    "    model = load_svm_model(nb_class=nb_class, weights_path=weights_path)\n",
    "\n",
    "    bottleneck_features_train = model.predict(X_train)\n",
    "    np.save(open('alex_bottleneck_svmfeatures_train'+ str(fold_count) +'.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "\n",
    "    bottleneck_features_validation = model.predict(X_test)\n",
    "    np.save(open('alex_bottleneck_svmfeatures_validation'+ str(fold_count) + '.npy', 'wb'), bottleneck_features_validation)\n",
    "    print \"deep features extracted\", bottleneck_features_train.shape[1:]\n",
    "\n",
    "# Train top model and save weithgs \n",
    "def train_top_model(y_train, y_test):\n",
    "    X_train = np.load(open('alex_bottleneck_features_validation'+ str(fold_count) + '.npy' , 'rb'))\n",
    "    X_test = np.load(open('alex_bottleneck_features_validation'+ str(fold_count) + '.npy', 'rb'))\n",
    "\n",
    "\n",
    "    print \"\\nTraining CNN..\"\n",
    "    y_train = np_utils.to_categorical(y_train, nb_class)\n",
    "    y_test = np_utils.to_categorical(y_test, nb_class)\n",
    "\n",
    "    shape=X_train.shape[1:]\n",
    "    \n",
    "    model = None # Clear Model\n",
    "\n",
    "    model = get_top_model_for_alexnet(\n",
    "        shape=shape,\n",
    "        nb_class=nb_class)\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    hist = model.fit(X_train, y_train,\n",
    "              nb_epoch=nb_epoch, batch_size=32,verbose=1)\n",
    "    \n",
    "    #visualize_loss(hist) # Removing this because we not training CNN, just classification \n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    model.save_weights(\"model/alex_topmodel\" + str(fold_count) + \".h5\")\n",
    "    print(\"CNN %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "    return scores[1]\n",
    "\n",
    "# SVM classification \n",
    "def train_svm(y_train, y_test):\n",
    "    svm_train = np.load(open('alex_bottleneck_svmfeatures_train'+ str(fold_count) +'.npy' , 'rb'))\n",
    "    svm_test = np.load(open('alex_bottleneck_svmfeatures_validation'+ str(fold_count) + '.npy', 'rb'))\n",
    "\n",
    "\n",
    "    print \"\\nTraining SVM..\"\n",
    "    clf = svm.SVC(kernel='linear', gamma=0.7, C=1.0)\n",
    "\n",
    "    clf.fit(svm_train, y_train.ravel())\n",
    "    #y_pred = clf.predict(test_data)\n",
    "    score = clf.score(svm_test, y_test.ravel())\n",
    "    print(\"SVM %s: %.2f%%\" % (\"acc: \", score*100))\n",
    "    \n",
    "    y_pred = clf.predict(svm_test)\n",
    "    \n",
    "    target_names = ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "     'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    "     'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    "     'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    "     'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    "     'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    "     'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    "     'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    "     'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    "     'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    "     'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    "     'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    "     'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    "     'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    "     'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    "     'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    "     'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    "     'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng']\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(classification_report(y_test, y_pred,target_names=target_names))\n",
    "    print(cm)\n",
    "    \n",
    "    return score\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    n_folds = 2\n",
    "    total_scores = 0\n",
    "    \n",
    "    print \"Loading data..\"\n",
    "    data, labels, lz = load_data()\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    lz = np.array(lz)\n",
    "    print lz.shape\n",
    "    print \"Data loaded !\"\n",
    "    \n",
    "    skf = StratifiedKFold(y=lz, n_folds=n_folds, shuffle=False)\n",
    "    \n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print \"Test train Shape: \"\n",
    "        print data[train].shape\n",
    "        print data[test].shape\n",
    "        print (\"Running Fold %d / %d\" % (i+1, n_folds))\n",
    "        \n",
    "        save_bottleneck_features(data[train], data[test],labels[train], labels[test])\n",
    "        scores = train_top_model(labels[train], labels[test])\n",
    "        \n",
    "        save_bottleneck_svmfeatures(data[train], data[test],labels[train], labels[test])\n",
    "        svm_scores = train_svm(labels[train], labels[test])\n",
    "        \n",
    "        total_scores = total_scores + svm_scores\n",
    "        fold_count = fold_count + 1\n",
    "    print(\"Average acc : %.2f%%\" % (total_scores/n_folds*100))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
