{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is disabled, cuDNN 5105)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "Data loaded !\n",
      "Running Fold 1 / 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/machine/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode='max')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNN..\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.6057 - acc: 0.0074 - val_loss: 4.6048 - val_acc: 0.0099\n",
      "Epoch 2/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.6054 - acc: 0.0090 - val_loss: 4.6044 - val_acc: 0.0099\n",
      "Epoch 3/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.6049 - acc: 0.0113 - val_loss: 4.6038 - val_acc: 0.0100\n",
      "Epoch 4/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.6043 - acc: 0.0106 - val_loss: 4.6028 - val_acc: 0.0188\n",
      "Epoch 5/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.6032 - acc: 0.0133 - val_loss: 4.6004 - val_acc: 0.0199\n",
      "Epoch 6/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.5998 - acc: 0.0133 - val_loss: 4.5934 - val_acc: 0.0233\n",
      "Epoch 7/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.5820 - acc: 0.0157 - val_loss: 4.5458 - val_acc: 0.0222\n",
      "Epoch 8/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.4922 - acc: 0.0206 - val_loss: 4.5275 - val_acc: 0.0204\n",
      "Epoch 9/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.4110 - acc: 0.0236 - val_loss: 4.3655 - val_acc: 0.0281\n",
      "Epoch 10/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.3623 - acc: 0.0289 - val_loss: 4.2701 - val_acc: 0.0364\n",
      "Epoch 11/200\n",
      "10000/10000 [==============================] - 55s - loss: 4.3208 - acc: 0.0305 - val_loss: 4.3025 - val_acc: 0.0419\n",
      "Epoch 12/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.2810 - acc: 0.0336 - val_loss: 4.2130 - val_acc: 0.0438\n",
      "Epoch 13/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.2397 - acc: 0.0381 - val_loss: 4.1488 - val_acc: 0.0504\n",
      "Epoch 14/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.1954 - acc: 0.0449 - val_loss: 4.1176 - val_acc: 0.0555\n",
      "Epoch 15/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.1588 - acc: 0.0492 - val_loss: 4.0920 - val_acc: 0.0528\n",
      "Epoch 16/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.1113 - acc: 0.0557 - val_loss: 4.0885 - val_acc: 0.0680\n",
      "Epoch 17/200\n",
      "10000/10000 [==============================] - 54s - loss: 4.0520 - acc: 0.0657 - val_loss: 4.0296 - val_acc: 0.0751\n",
      "Epoch 18/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.9928 - acc: 0.0754 - val_loss: 3.9218 - val_acc: 0.0856\n",
      "Epoch 19/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.9385 - acc: 0.0797 - val_loss: 3.9221 - val_acc: 0.0847\n",
      "Epoch 20/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.8823 - acc: 0.0860 - val_loss: 3.8008 - val_acc: 0.0939\n",
      "Epoch 21/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.8195 - acc: 0.0983 - val_loss: 3.8031 - val_acc: 0.1027\n",
      "Epoch 22/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.7765 - acc: 0.1000 - val_loss: 3.7773 - val_acc: 0.1037\n",
      "Epoch 23/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.7287 - acc: 0.1119 - val_loss: 3.7033 - val_acc: 0.1216\n",
      "Epoch 24/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.6680 - acc: 0.1239 - val_loss: 3.6304 - val_acc: 0.1309\n",
      "Epoch 25/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.5923 - acc: 0.1360 - val_loss: 3.9107 - val_acc: 0.0995\n",
      "Epoch 26/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.5336 - acc: 0.1437 - val_loss: 3.8886 - val_acc: 0.0979\n",
      "Epoch 27/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.4789 - acc: 0.1524 - val_loss: 3.4821 - val_acc: 0.1553\n",
      "Epoch 28/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.4153 - acc: 0.1651 - val_loss: 3.4526 - val_acc: 0.1569\n",
      "Epoch 29/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.3511 - acc: 0.1736 - val_loss: 3.4659 - val_acc: 0.1657\n",
      "Epoch 30/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.2738 - acc: 0.1912 - val_loss: 3.4175 - val_acc: 0.1701\n",
      "Epoch 31/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.2136 - acc: 0.1928 - val_loss: 3.5091 - val_acc: 0.1595\n",
      "Epoch 32/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.1458 - acc: 0.2149 - val_loss: 3.3704 - val_acc: 0.1764\n",
      "Epoch 33/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.0634 - acc: 0.2233 - val_loss: 3.4856 - val_acc: 0.1693\n",
      "Epoch 34/200\n",
      "10000/10000 [==============================] - 54s - loss: 3.0018 - acc: 0.2373 - val_loss: 3.2459 - val_acc: 0.2083\n",
      "Epoch 35/200\n",
      "10000/10000 [==============================] - 54s - loss: 2.9179 - acc: 0.2527 - val_loss: 3.5457 - val_acc: 0.1644\n",
      "Epoch 36/200\n",
      "10000/10000 [==============================] - 54s - loss: 2.8328 - acc: 0.2758 - val_loss: 3.2606 - val_acc: 0.2070\n",
      "Epoch 37/200\n",
      "10000/10000 [==============================] - 54s - loss: 2.7506 - acc: 0.2824 - val_loss: 3.2234 - val_acc: 0.2120\n",
      "Epoch 38/200\n",
      "10000/10000 [==============================] - 54s - loss: 2.6351 - acc: 0.3142 - val_loss: 3.4622 - val_acc: 0.1862\n",
      "Epoch 39/200\n",
      "10000/10000 [==============================] - 54s - loss: 2.5387 - acc: 0.3284 - val_loss: 3.2778 - val_acc: 0.2124\n",
      "Epoch 40/200\n",
      "10000/10000 [==============================] - 54s - loss: 2.4294 - acc: 0.3526 - val_loss: 3.4665 - val_acc: 0.1913\n",
      "Epoch 41/200\n",
      "10000/10000 [==============================] - 54s - loss: 2.3161 - acc: 0.3776 - val_loss: 3.5692 - val_acc: 0.1916\n",
      "Epoch 42/200\n",
      "10000/10000 [==============================] - 54s - loss: 2.1967 - acc: 0.4096 - val_loss: 3.1618 - val_acc: 0.2381\n",
      "Epoch 43/200\n",
      "10000/10000 [==============================] - 54s - loss: 2.0779 - acc: 0.4337 - val_loss: 3.3144 - val_acc: 0.2202\n",
      "Epoch 44/200\n",
      "10000/10000 [==============================] - 54s - loss: 1.9212 - acc: 0.4708 - val_loss: 3.2172 - val_acc: 0.2452\n",
      "Epoch 45/200\n",
      " 9984/10000 [============================>.] - ETA: 0s - loss: 1.8090 - acc: 0.4980"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Reshape, Permute, Activation, \\\n",
    "Input, merge\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "from convnetskeras.customlayers import convolution2Dgroup, crosschannelnormalization, \\\n",
    "splittensor, Softmax4D\n",
    "\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "from sklearn import svm\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "import sys, glob, argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import hickle as hkl\n",
    "\n",
    "batch_size = 32\n",
    "nb_classes = 100\n",
    "nb_epoch = 200\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 227,227\n",
    "img_channels = 3\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.jet):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(10)\n",
    "    plt.xticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    " 'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    " 'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi'])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def visualize_loss(hist):\n",
    "    train_loss=hist.history['loss']\n",
    "    val_loss=hist.history['val_loss']\n",
    "    train_acc=hist.history['acc']\n",
    "    val_acc=hist.history['val_acc']\n",
    "    xc=range(nb_epoch)\n",
    "\n",
    "    plt.figure(1,figsize=(7,5))\n",
    "    plt.plot(xc,train_loss)\n",
    "    plt.plot(xc,val_loss)\n",
    "    plt.xlabel('num of Epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('train_loss vs val_loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['train','val'])\n",
    "    #print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "    plt.style.use(['classic'])\n",
    "\n",
    "    plt.figure(2,figsize=(7,5))\n",
    "    plt.plot(xc,train_acc)\n",
    "    plt.plot(xc,val_acc)\n",
    "    plt.xlabel('num of Epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('train_acc vs val_acc')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['train','val'],loc=4)\n",
    "    #print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "    plt.style.use(['classic'])\n",
    "    plt.show()\n",
    "\n",
    "def load_data():\n",
    "    # load your data using this function\n",
    "    d = hkl.load('../dataset/myfood100-227.hkl')\n",
    "    data = d['trainFeatures']\n",
    "    labels = d['trainLabels']\n",
    "    lz = d['labels']\n",
    "    data = data.reshape(data.shape[0], 3, 227, 227)\n",
    "    #data = data.transpose(0, 2, 3, 1)\n",
    "\n",
    "    return data,labels,lz\n",
    "\n",
    "def load_model(nb_class, weights_path=None):\n",
    "\n",
    "    inputs = Input(shape=(3,227,227))\n",
    "\n",
    "    conv_1 = Convolution2D(96, 11, 11,subsample=(4,4),activation='relu',\n",
    "                           name='conv_1')(inputs)\n",
    "\n",
    "    conv_2 = MaxPooling2D((3, 3), strides=(2,2))(conv_1)\n",
    "    conv_2 = crosschannelnormalization(name=\"convpool_1\")(conv_2)\n",
    "    conv_2 = ZeroPadding2D((2,2))(conv_2)\n",
    "    conv_2 = merge([\n",
    "        Convolution2D(128,5,5,activation=\"relu\",name='conv_2_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_2)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_2\")\n",
    "\n",
    "    conv_3 = MaxPooling2D((3, 3), strides=(2, 2))(conv_2)\n",
    "    conv_3 = crosschannelnormalization()(conv_3)\n",
    "    conv_3 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_3 = Convolution2D(384,3,3,activation='relu',name='conv_3')(conv_3)\n",
    "\n",
    "    conv_4 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_4 = merge([\n",
    "        Convolution2D(192,3,3,activation=\"relu\",name='conv_4_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_4)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_4\")\n",
    "\n",
    "    conv_5 = ZeroPadding2D((1,1))(conv_4)\n",
    "    conv_5 = merge([\n",
    "        Convolution2D(128,3,3,activation=\"relu\",name='conv_5_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_5)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_5\")\n",
    "\n",
    "    conv_5 = MaxPooling2D((3, 3), strides=(2,2),name=\"convpool_5\")(conv_5)\n",
    "\n",
    "\n",
    "\n",
    "    dense_1 = Flatten(name=\"flatten\")(conv_5)\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1')(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2')(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(nb_class,name='dense_3')(dense_3)\n",
    "    prediction = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "\n",
    "\n",
    "    base_model = Model(input=inputs, output=prediction)\n",
    "\n",
    "    if weights_path:\n",
    "        base_model.load_weights(weights_path)\n",
    "\n",
    "    base_model = Model(input=inputs, output=dense_2)\n",
    "\n",
    "    return base_model\n",
    "\n",
    "def create_model(weights_path=None, heatmap=False):\n",
    "\n",
    "    inputs = Input(shape=(3,227,227))\n",
    "\n",
    "    conv_1 = Convolution2D(96, 11, 11,subsample=(4,4),activation='relu',\n",
    "                           name='conv_1')(inputs)\n",
    "\n",
    "    conv_2 = MaxPooling2D((3, 3), strides=(2,2))(conv_1)\n",
    "    conv_2 = crosschannelnormalization(name=\"convpool_1\")(conv_2)\n",
    "    conv_2 = ZeroPadding2D((2,2))(conv_2)\n",
    "    conv_2 = merge([\n",
    "        Convolution2D(128,5,5,activation=\"relu\",name='conv_2_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_2)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_2\")\n",
    "\n",
    "    conv_3 = MaxPooling2D((3, 3), strides=(2, 2))(conv_2)\n",
    "    conv_3 = crosschannelnormalization()(conv_3)\n",
    "    conv_3 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_3 = Convolution2D(384,3,3,activation='relu',name='conv_3')(conv_3)\n",
    "\n",
    "    conv_4 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_4 = merge([\n",
    "        Convolution2D(192,3,3,activation=\"relu\",name='conv_4_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_4)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_4\")\n",
    "\n",
    "    conv_5 = ZeroPadding2D((1,1))(conv_4)\n",
    "    conv_5 = merge([\n",
    "        Convolution2D(128,3,3,activation=\"relu\",name='conv_5_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_5)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_5\")\n",
    "\n",
    "    dense_1 = MaxPooling2D((3, 3), strides=(2,2),name=\"convpool_5\")(conv_5)\n",
    "\n",
    "\n",
    "    dense_1 = Flatten(name=\"flatten\")(dense_1)\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1')(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2')(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(nb_classes,name='dense_3')(dense_3)\n",
    "    prediction = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "\n",
    "\n",
    "    model = Model(input=inputs, output=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    opt = SGD(lr=0.01)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])\n",
    "    #rms = RMSprop(lr=5e-4, rho=0.9, epsilon=1e-08, decay=0.01)\n",
    "    #model.compile(optimizer=rms, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    \n",
    "    print \"Training CNN..\"\n",
    "    hist = model.fit(X_train, Y_train, nb_epoch=nb_epoch, validation_data=(X_test, Y_test),batch_size=batch_size,verbose=1)\n",
    "    visualize_loss(hist)\n",
    "    model.save_weights(\"food10_scratch_weights.h5\")\n",
    "    \n",
    "    scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Softmax %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    \n",
    "    \n",
    "    model = None\n",
    "    model = load_model(nb_class=nb_classes, weights_path=\"food10_scratch_weights.h5\")\n",
    "    \n",
    "    svm_train = model.predict(X_train, batch_size=32)\n",
    "    svm_test = model.predict(X_test, batch_size=32)\n",
    "    \n",
    "    print \"\\nTraining SVM..\"\n",
    "    clf = svm.SVC(kernel='linear', gamma=0.7, C=1.0)\n",
    "\n",
    "    clf.fit(svm_train, y_train.ravel())\n",
    "    #y_pred = clf.predict(test_data)\n",
    "    score = clf.score(svm_test, y_test.ravel())\n",
    "    print(\"SVM %s: %.2f%%\" % (\"acc: \", score*100))\n",
    "    \n",
    "    y_pred = clf.predict(svm_test)\n",
    "    target_names = ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "                     'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi']\n",
    "    cm = confusion_matrix(y_test.ravel(), y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Visualization of confusion matrix\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm)\n",
    "    plt.show()\n",
    "   \n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    #cm = confusion_matrix(np.argmax(y_test,axis=1), y_pred)\n",
    "    #print cm\n",
    "    #np.set_printoptions(precision=2)\n",
    "    #plt.figure()\n",
    "    #plot_confusion_matrix(cm)\n",
    "    #plt.show()\n",
    "\n",
    "    return score\n",
    "\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    n_folds = 2\n",
    "    print \"Loading data..\"\n",
    "    data, labels, lz = load_data()\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    lz = np.array(lz)\n",
    "    print \"Data loaded !\"\n",
    "    total_scores = 0 \n",
    "\n",
    "\n",
    "    skf = StratifiedKFold(y=lz, n_folds=n_folds, shuffle=True)\n",
    "\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print (\"Running Fold %d / %d\" % (i+1, n_folds))\n",
    "        model = None # Clearing the NN.\n",
    "        model = create_model(weights_path=None, heatmap=False)\n",
    "        scores = train_and_evaluate_model(model, data[train], labels[train], data[test], labels[test])\n",
    "        #print \"Score for fold \", i+1, \"= \", scores*100\n",
    "        total_scores = total_scores + scores\n",
    "\n",
    "    print(\"Average acc : %.2f%%\" % (total_scores/n_folds*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
