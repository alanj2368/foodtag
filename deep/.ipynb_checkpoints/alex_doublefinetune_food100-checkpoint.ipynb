{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is disabled, cuDNN 5105)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "(20000,)\n",
      "Data loaded !\n",
      "Test train Shape: \n",
      "(10000, 3, 227, 227)\n",
      "(10000, 3, 227, 227)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/machine/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode='max')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Fold 1 / 2\n",
      "<keras.engine.topology.InputLayer object at 0x7fd825a1f750>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fd8259f1e90>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fd2632e5b10>\n",
      "<keras.layers.core.Lambda object at 0x7fd263294350>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fd2631adf50>\n",
      "<keras.layers.core.Lambda object at 0x7fd2631c39d0>\n",
      "<keras.layers.core.Lambda object at 0x7fd26317d1d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fd2631c3950>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fd26317d150>\n",
      "<keras.engine.topology.Merge object at 0x7fd2631c3910>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fd26319fa10>\n",
      "<keras.layers.core.Lambda object at 0x7fd26319ff50>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fd263162110>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fd2630f2890>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fd263122150>\n",
      "<keras.layers.core.Lambda object at 0x7fd263105350>\n",
      "<keras.layers.core.Lambda object at 0x7fd2630d53d0>\n",
      "Weights for \"dense_1\" are loaded\n",
      "Weights for \"dense_2\" are loaded\n",
      "Weights for \"dense_3\" are loaded\n",
      "\n",
      "Training CNN..\n",
      "Epoch 1/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.8610 - acc: 0.3070    \n",
      "Epoch 2/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.7064 - acc: 0.3294    \n",
      "Epoch 3/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.5956 - acc: 0.3454    \n",
      "Epoch 4/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.4979 - acc: 0.3683    \n",
      "Epoch 5/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.4227 - acc: 0.3817    \n",
      "Epoch 6/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.3804 - acc: 0.3889    \n",
      "Epoch 7/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.3122 - acc: 0.4028    \n",
      "Epoch 8/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.2644 - acc: 0.4121    \n",
      "Epoch 9/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.2131 - acc: 0.4194    \n",
      "Epoch 10/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.1691 - acc: 0.4292    \n",
      "Epoch 11/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.1001 - acc: 0.4483    \n",
      "Epoch 12/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.0592 - acc: 0.4585    \n",
      "Epoch 13/150\n",
      "10000/10000 [==============================] - 26s - loss: 2.0414 - acc: 0.4620    \n",
      "Epoch 14/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.9757 - acc: 0.4694    \n",
      "Epoch 15/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.9372 - acc: 0.4815    \n",
      "Epoch 16/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.9084 - acc: 0.4923    \n",
      "Epoch 17/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.8575 - acc: 0.5065    \n",
      "Epoch 18/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.8311 - acc: 0.5028    \n",
      "Epoch 19/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.8038 - acc: 0.5101    \n",
      "Epoch 20/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.7477 - acc: 0.5243    \n",
      "Epoch 21/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.7221 - acc: 0.5337    \n",
      "Epoch 22/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.6931 - acc: 0.5409    \n",
      "Epoch 23/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.6644 - acc: 0.5477    \n",
      "Epoch 24/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.6185 - acc: 0.5569    \n",
      "Epoch 25/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.5876 - acc: 0.5659    \n",
      "Epoch 26/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.5712 - acc: 0.5692    \n",
      "Epoch 27/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.5302 - acc: 0.5803    \n",
      "Epoch 28/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.5174 - acc: 0.5810    \n",
      "Epoch 29/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.4666 - acc: 0.5936    \n",
      "Epoch 30/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.4427 - acc: 0.5997    \n",
      "Epoch 31/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.4165 - acc: 0.6077    \n",
      "Epoch 32/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.3819 - acc: 0.6181    \n",
      "Epoch 33/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.3548 - acc: 0.6256    \n",
      "Epoch 34/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.3379 - acc: 0.6305    \n",
      "Epoch 35/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.2918 - acc: 0.6381    \n",
      "Epoch 36/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.2724 - acc: 0.6423    \n",
      "Epoch 37/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.2554 - acc: 0.6507    \n",
      "Epoch 38/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.2239 - acc: 0.6572    \n",
      "Epoch 39/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.1961 - acc: 0.6671    \n",
      "Epoch 40/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.1689 - acc: 0.6720    \n",
      "Epoch 41/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.1386 - acc: 0.6820    \n",
      "Epoch 42/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.1258 - acc: 0.6797    \n",
      "Epoch 43/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.0946 - acc: 0.6891    \n",
      "Epoch 44/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.0596 - acc: 0.7001    \n",
      "Epoch 45/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.0554 - acc: 0.7031    \n",
      "Epoch 46/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.0282 - acc: 0.7058    \n",
      "Epoch 47/150\n",
      "10000/10000 [==============================] - 26s - loss: 1.0031 - acc: 0.7172    \n",
      "Epoch 48/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.9826 - acc: 0.7216    \n",
      "Epoch 49/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.9515 - acc: 0.7309    \n",
      "Epoch 50/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.9452 - acc: 0.7320    \n",
      "Epoch 51/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.9108 - acc: 0.7430    \n",
      "Epoch 52/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.8961 - acc: 0.7464    \n",
      "Epoch 53/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.8794 - acc: 0.7520    \n",
      "Epoch 54/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.8597 - acc: 0.7562    \n",
      "Epoch 55/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.8326 - acc: 0.7681    \n",
      "Epoch 56/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.8142 - acc: 0.7705    \n",
      "Epoch 57/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.8099 - acc: 0.7698    \n",
      "Epoch 58/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.7655 - acc: 0.7850    \n",
      "Epoch 59/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.7551 - acc: 0.7849    \n",
      "Epoch 60/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.7529 - acc: 0.7885    \n",
      "Epoch 61/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.7243 - acc: 0.7972    \n",
      "Epoch 62/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.7097 - acc: 0.8016    \n",
      "Epoch 63/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.6965 - acc: 0.8047    \n",
      "Epoch 64/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.6724 - acc: 0.8089    \n",
      "Epoch 65/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.6673 - acc: 0.8126    \n",
      "Epoch 66/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.6374 - acc: 0.8239    \n",
      "Epoch 67/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.6424 - acc: 0.8188    \n",
      "Epoch 68/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.6215 - acc: 0.8279    \n",
      "Epoch 69/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.6075 - acc: 0.8305    \n",
      "Epoch 70/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.5900 - acc: 0.8386    \n",
      "Epoch 71/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.5686 - acc: 0.8465    \n",
      "Epoch 72/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.5610 - acc: 0.8418    \n",
      "Epoch 73/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.5521 - acc: 0.8470    \n",
      "Epoch 74/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.5377 - acc: 0.8487    \n",
      "Epoch 75/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.5274 - acc: 0.8542    \n",
      "Epoch 76/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.5010 - acc: 0.8618    \n",
      "Epoch 77/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.4932 - acc: 0.8633    \n",
      "Epoch 78/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.4939 - acc: 0.8649    \n",
      "Epoch 79/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.4724 - acc: 0.8698    \n",
      "Epoch 80/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.4609 - acc: 0.8788    \n",
      "Epoch 81/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.4530 - acc: 0.8777    \n",
      "Epoch 82/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.4359 - acc: 0.8826    \n",
      "Epoch 83/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.4273 - acc: 0.8895    \n",
      "Epoch 84/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.4235 - acc: 0.8867    \n",
      "Epoch 85/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.4099 - acc: 0.8889    \n",
      "Epoch 86/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.4027 - acc: 0.8934    \n",
      "Epoch 87/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3994 - acc: 0.8943    \n",
      "Epoch 88/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3951 - acc: 0.8926    \n",
      "Epoch 89/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3820 - acc: 0.8966    \n",
      "Epoch 90/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3664 - acc: 0.9018    \n",
      "Epoch 91/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3634 - acc: 0.9080    \n",
      "Epoch 92/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3495 - acc: 0.9088    \n",
      "Epoch 93/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3378 - acc: 0.9104    \n",
      "Epoch 94/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3341 - acc: 0.9122    \n",
      "Epoch 95/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3275 - acc: 0.9167    \n",
      "Epoch 96/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3235 - acc: 0.9200    \n",
      "Epoch 97/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3171 - acc: 0.9173    \n",
      "Epoch 98/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3097 - acc: 0.9206    \n",
      "Epoch 99/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3021 - acc: 0.9253    \n",
      "Epoch 100/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.3050 - acc: 0.9181    \n",
      "Epoch 101/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2897 - acc: 0.9244    \n",
      "Epoch 102/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2897 - acc: 0.9243    \n",
      "Epoch 103/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2779 - acc: 0.9271    \n",
      "Epoch 104/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2594 - acc: 0.9356    \n",
      "Epoch 105/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2644 - acc: 0.9335    \n",
      "Epoch 106/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2558 - acc: 0.9369    \n",
      "Epoch 107/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2581 - acc: 0.9353    \n",
      "Epoch 108/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2419 - acc: 0.9415    \n",
      "Epoch 109/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2427 - acc: 0.9405    \n",
      "Epoch 110/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2330 - acc: 0.9432    \n",
      "Epoch 111/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2330 - acc: 0.9428    \n",
      "Epoch 112/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2368 - acc: 0.9385    \n",
      "Epoch 113/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2306 - acc: 0.9424    \n",
      "Epoch 114/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2194 - acc: 0.9464    \n",
      "Epoch 115/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2195 - acc: 0.9468    \n",
      "Epoch 116/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2205 - acc: 0.9440    \n",
      "Epoch 117/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2130 - acc: 0.9492    \n",
      "Epoch 118/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2034 - acc: 0.9526    \n",
      "Epoch 119/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.2023 - acc: 0.9521    \n",
      "Epoch 120/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1921 - acc: 0.9535    \n",
      "Epoch 121/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1973 - acc: 0.9542    \n",
      "Epoch 122/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1941 - acc: 0.9537    \n",
      "Epoch 123/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1893 - acc: 0.9568    \n",
      "Epoch 124/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1825 - acc: 0.9586    \n",
      "Epoch 125/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1810 - acc: 0.9578    \n",
      "Epoch 126/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1794 - acc: 0.9597    \n",
      "Epoch 127/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1710 - acc: 0.9599    \n",
      "Epoch 128/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1726 - acc: 0.9586    \n",
      "Epoch 129/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1751 - acc: 0.9574    \n",
      "Epoch 130/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1615 - acc: 0.9644    \n",
      "Epoch 131/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1561 - acc: 0.9650    \n",
      "Epoch 132/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1559 - acc: 0.9646    \n",
      "Epoch 133/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1591 - acc: 0.9613    \n",
      "Epoch 134/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1570 - acc: 0.9638    \n",
      "Epoch 135/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1572 - acc: 0.9617    \n",
      "Epoch 136/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1525 - acc: 0.9668    \n",
      "Epoch 137/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1510 - acc: 0.9641    \n",
      "Epoch 138/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1510 - acc: 0.9649    \n",
      "Epoch 139/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1420 - acc: 0.9684    \n",
      "Epoch 140/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1397 - acc: 0.9707    \n",
      "Epoch 141/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1465 - acc: 0.9665    \n",
      "Epoch 142/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1393 - acc: 0.9684    \n",
      "Epoch 143/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1322 - acc: 0.9715    \n",
      "Epoch 144/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1403 - acc: 0.9668    \n",
      "Epoch 145/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1313 - acc: 0.9718    \n",
      "Epoch 146/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1284 - acc: 0.9728    \n",
      "Epoch 147/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1281 - acc: 0.9709    \n",
      "Epoch 148/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1301 - acc: 0.9709    \n",
      "Epoch 149/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1252 - acc: 0.9717    \n",
      "Epoch 150/150\n",
      "10000/10000 [==============================] - 26s - loss: 0.1197 - acc: 0.9735    \n",
      "CNN acc: 74.96%\n",
      "deep features extracted (4096,)\n",
      "\n",
      "Training SVM..\n",
      "SVM acc: : 66.64%\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "           AisKacang       0.69      0.76      0.72       100\n",
      "           AngKuKueh       0.50      0.51      0.51       100\n",
      "           ApamBalik       0.67      0.79      0.72       100\n",
      "           Asamlaksa       0.56      0.63      0.59       100\n",
      "              Bahulu       0.85      0.87      0.86       100\n",
      "           Bakkukteh       0.78      0.84      0.81       100\n",
      "      BananaLeafRice       0.74      0.73      0.73       100\n",
      "             Bazhang       0.65      0.60      0.63       100\n",
      "         BeefRendang       0.90      0.94      0.92       100\n",
      "           BingkaUbi       0.76      0.87      0.81       100\n",
      "         Buburchacha       0.66      0.66      0.66       100\n",
      "          Buburpedas       0.66      0.77      0.71       100\n",
      "              Capati       0.59      0.61      0.60       100\n",
      "              Cendol       0.64      0.67      0.65       100\n",
      "         ChaiTowKuay       0.80      0.86      0.83       100\n",
      "        CharKuehTiao       0.78      0.81      0.79       100\n",
      "             CharSiu       0.78      0.78      0.78       100\n",
      "       CheeCheongFun       0.83      0.94      0.88       100\n",
      "           ChiliCrab       0.54      0.51      0.53       100\n",
      "           Chweekueh       0.75      0.79      0.77       100\n",
      "         ClayPotRice       0.47      0.47      0.47       100\n",
      "          CucurUdang       0.68      0.50      0.58       100\n",
      "          CurryLaksa       0.47      0.49      0.48       100\n",
      "           CurryPuff       0.58      0.59      0.59       100\n",
      "               Dodol       0.60      0.75      0.67       100\n",
      "              Durian       0.54      0.63      0.58       100\n",
      "         DurianCrepe       0.60      0.68      0.64       100\n",
      "       FishHeadCurry       0.47      0.44      0.46       100\n",
      "               Guava       0.56      0.59      0.57       100\n",
      "HainaneseChickenRice       0.51      0.49      0.50       100\n",
      "          HokkienMee       0.63      0.64      0.63       100\n",
      "            Huatkuih       0.91      0.77      0.83       100\n",
      "           IkanBakar       0.85      0.76      0.80       100\n",
      "            Kangkung       0.70      0.71      0.70       100\n",
      "           KayaToast       0.51      0.55      0.53       100\n",
      "            Keklapis       0.49      0.58      0.53       100\n",
      "             Ketupat       0.68      0.70      0.69       100\n",
      "           KuihDadar       0.50      0.59      0.54       100\n",
      "           KuihLapis       0.78      0.86      0.82       100\n",
      "        KuihSeriMuka       0.60      0.54      0.57       100\n",
      "             Langsat       0.76      0.76      0.76       100\n",
      "               Lekor       0.57      0.55      0.56       100\n",
      "              Lemang       0.78      0.70      0.74       100\n",
      "         LepatPisang       0.50      0.54      0.52       100\n",
      "              LorMee       0.73      0.74      0.74       100\n",
      "        Maggi goreng       0.83      0.82      0.82       100\n",
      "          Mangosteen       0.65      0.63      0.64       100\n",
      "           MeeGoreng       0.62      0.61      0.61       100\n",
      "         MeeHoonKueh       0.79      0.71      0.75       100\n",
      "         MeeHoonSoup       0.48      0.51      0.49       100\n",
      "             MeeJawa       0.78      0.80      0.79       100\n",
      "            MeeRebus       0.69      0.67      0.68       100\n",
      "            MeeRojak       0.68      0.71      0.70       100\n",
      "             MeeSiam       0.73      0.71      0.72       100\n",
      "            Murtabak       0.71      0.68      0.69       100\n",
      "             Murukku       0.73      0.71      0.72       100\n",
      "   NasiGorengKampung       0.70      0.70      0.70       100\n",
      "           NasiImpit       0.71      0.78      0.74       100\n",
      "          Nasikandar       0.76      0.79      0.77       100\n",
      "           Nasilemak       0.53      0.43      0.48       100\n",
      "         Nasipattaya       0.76      0.68      0.72       100\n",
      "          Ondehondeh       0.56      0.55      0.55       100\n",
      "            Otakotak       0.64      0.78      0.71       100\n",
      "      OysterOmelette       0.64      0.69      0.67       100\n",
      "              PanMee       0.56      0.58      0.57       100\n",
      "       PineappleTart       0.53      0.49      0.51       100\n",
      "        PisangGoreng       0.80      0.73      0.76       100\n",
      "              Popiah       0.66      0.79      0.72       100\n",
      "            PrawnMee       0.54      0.65      0.59       100\n",
      "         Prawnsambal       0.64      0.66      0.65       100\n",
      "                Puri       0.78      0.69      0.73       100\n",
      "           PutuMayam       0.66      0.57      0.61       100\n",
      "          PutuPiring       0.50      0.42      0.46       100\n",
      "            Rambutan       0.71      0.62      0.66       100\n",
      "               Rojak       0.85      0.94      0.90       100\n",
      "           RotiCanai       0.67      0.55      0.60       100\n",
      "            RotiJala       0.60      0.64      0.62       100\n",
      "            RotiJohn       0.76      0.87      0.81       100\n",
      "            RotiNaan       0.59      0.63      0.61       100\n",
      "          RotiTissue       0.59      0.46      0.52       100\n",
      "         SambalPetai       0.66      0.73      0.69       100\n",
      "         SambalUdang       0.59      0.47      0.53       100\n",
      "               Satay       0.68      0.59      0.63       100\n",
      "          Sataycelup       0.74      0.64      0.68       100\n",
      "            SeriMuka       0.94      0.93      0.93       100\n",
      "            SotoAyam       0.76      0.71      0.73       100\n",
      "     TandooriChicken       0.54      0.48      0.51       100\n",
      "            TangYuan       0.72      0.67      0.69       100\n",
      "           TauFooFah       0.84      0.89      0.86       100\n",
      "         TauhuSumbat       0.61      0.46      0.53       100\n",
      "              Thosai       0.66      0.59      0.62       100\n",
      "          TomYumSoup       0.87      0.89      0.88       100\n",
      "               Wajik       0.47      0.46      0.46       100\n",
      "           WanTanMee       0.66      0.71      0.69       100\n",
      "             WaTanHo       0.63      0.63      0.63       100\n",
      "              Wonton       0.61      0.52      0.56       100\n",
      "             YamCake       0.54      0.49      0.51       100\n",
      "           YongTauFu       0.81      0.81      0.81       100\n",
      "             Youtiao       0.54      0.48      0.51       100\n",
      "             Yusheng       0.77      0.68      0.72       100\n",
      "\n",
      "         avg / total       0.67      0.67      0.66     10000\n",
      "\n",
      "[[76  0  0 ...,  0  0  0]\n",
      " [ 0 51  1 ...,  0  0  0]\n",
      " [ 0  0 79 ...,  1  0  0]\n",
      " ..., \n",
      " [ 0  0  0 ..., 81  0  0]\n",
      " [ 0  0  0 ...,  1 48  0]\n",
      " [ 2  0  1 ...,  0  0 68]]\n",
      "Test train Shape: \n",
      "(10000, 3, 227, 227)\n",
      "(10000, 3, 227, 227)\n",
      "Running Fold 2 / 2\n",
      "<keras.engine.topology.InputLayer object at 0x7fd25079dc90>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fd2503a5bd0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fd250abf190>\n",
      "<keras.layers.core.Lambda object at 0x7fd2504d3650>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fd24fe2e390>\n",
      "<keras.layers.core.Lambda object at 0x7fd24fe9d2d0>\n",
      "<keras.layers.core.Lambda object at 0x7fd2504c6b90>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fd24fe9d810>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fd2504c6c50>\n",
      "<keras.engine.topology.Merge object at 0x7fd24fe9d4d0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fd250ac5290>\n",
      "<keras.layers.core.Lambda object at 0x7fd250ac55d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fd24f6bb6d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fd24fe9d8d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fd24f6fbe90>\n",
      "<keras.layers.core.Lambda object at 0x7fd24f707810>\n",
      "<keras.layers.core.Lambda object at 0x7fd24f751810>\n",
      "Weights for \"dense_1\" are loaded\n",
      "Weights for \"dense_2\" are loaded\n",
      "Weights for \"dense_3\" are loaded\n",
      "\n",
      "Training CNN..\n",
      "Epoch 1/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.8659 - acc: 0.2980    \n",
      "Epoch 2/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.6883 - acc: 0.3223    \n",
      "Epoch 3/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.5788 - acc: 0.3439    \n",
      "Epoch 4/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.5004 - acc: 0.3650    \n",
      "Epoch 5/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.4350 - acc: 0.3710    \n",
      "Epoch 6/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.3675 - acc: 0.3828    \n",
      "Epoch 7/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.3049 - acc: 0.3969    \n",
      "Epoch 8/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.2540 - acc: 0.4067    \n",
      "Epoch 9/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.2063 - acc: 0.4184    \n",
      "Epoch 10/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.1643 - acc: 0.4202    \n",
      "Epoch 11/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.1206 - acc: 0.4304    \n",
      "Epoch 12/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.0611 - acc: 0.4509    \n",
      "Epoch 13/150\n",
      "10000/10000 [==============================] - 25s - loss: 2.0243 - acc: 0.4581    \n",
      "Epoch 14/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.9822 - acc: 0.4660    \n",
      "Epoch 15/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.9493 - acc: 0.4750    \n",
      "Epoch 16/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.9047 - acc: 0.4788    \n",
      "Epoch 17/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.8716 - acc: 0.4880    \n",
      "Epoch 18/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.8267 - acc: 0.5007    \n",
      "Epoch 19/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.7890 - acc: 0.5069    \n",
      "Epoch 20/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.7675 - acc: 0.5128    \n",
      "Epoch 21/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.7325 - acc: 0.5251    \n",
      "Epoch 22/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.6977 - acc: 0.5346    \n",
      "Epoch 23/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.6691 - acc: 0.5371    \n",
      "Epoch 24/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.6284 - acc: 0.5488    \n",
      "Epoch 25/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.6071 - acc: 0.5513    \n",
      "Epoch 26/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.5537 - acc: 0.5680    \n",
      "Epoch 27/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.5253 - acc: 0.5699    \n",
      "Epoch 28/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.4842 - acc: 0.5824    \n",
      "Epoch 29/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.4620 - acc: 0.5932    \n",
      "Epoch 30/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.4249 - acc: 0.6017    \n",
      "Epoch 31/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.3905 - acc: 0.6070    \n",
      "Epoch 32/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.3930 - acc: 0.6052    \n",
      "Epoch 33/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.3588 - acc: 0.6177    \n",
      "Epoch 34/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.3239 - acc: 0.6301    \n",
      "Epoch 35/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.2749 - acc: 0.6371    \n",
      "Epoch 36/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.2530 - acc: 0.6436    \n",
      "Epoch 37/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.2289 - acc: 0.6490    \n",
      "Epoch 38/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.2067 - acc: 0.6603    \n",
      "Epoch 39/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.1770 - acc: 0.6650    \n",
      "Epoch 40/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.1607 - acc: 0.6651    \n",
      "Epoch 41/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.1374 - acc: 0.6777    \n",
      "Epoch 42/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.1171 - acc: 0.6844    \n",
      "Epoch 43/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.0844 - acc: 0.6920    \n",
      "Epoch 44/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.0635 - acc: 0.6970    \n",
      "Epoch 45/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.0279 - acc: 0.7086    \n",
      "Epoch 46/150\n",
      "10000/10000 [==============================] - 25s - loss: 1.0174 - acc: 0.7077    \n",
      "Epoch 47/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.9932 - acc: 0.7185    \n",
      "Epoch 48/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.9573 - acc: 0.7299    \n",
      "Epoch 49/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.9433 - acc: 0.7308    \n",
      "Epoch 50/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.9222 - acc: 0.7362    \n",
      "Epoch 51/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.9018 - acc: 0.7468    \n",
      "Epoch 52/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.8690 - acc: 0.7542    \n",
      "Epoch 53/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.8630 - acc: 0.7560    \n",
      "Epoch 54/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.8425 - acc: 0.7572    \n",
      "Epoch 55/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.8221 - acc: 0.7676    \n",
      "Epoch 56/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.8059 - acc: 0.7713    \n",
      "Epoch 57/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.7877 - acc: 0.7780    \n",
      "Epoch 58/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.7589 - acc: 0.7855    \n",
      "Epoch 59/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.7481 - acc: 0.7842    \n",
      "Epoch 60/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.7188 - acc: 0.7917    \n",
      "Epoch 61/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.7177 - acc: 0.7920    \n",
      "Epoch 62/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.6998 - acc: 0.8013    \n",
      "Epoch 63/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.6934 - acc: 0.8008    \n",
      "Epoch 64/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.6600 - acc: 0.8152    \n",
      "Epoch 65/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.6502 - acc: 0.8143    \n",
      "Epoch 66/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.6410 - acc: 0.8178    \n",
      "Epoch 67/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.6200 - acc: 0.8267    \n",
      "Epoch 68/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.5965 - acc: 0.8363    \n",
      "Epoch 69/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.5867 - acc: 0.8349    \n",
      "Epoch 70/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.5706 - acc: 0.8433    \n",
      "Epoch 71/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.5536 - acc: 0.8467    \n",
      "Epoch 72/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.5491 - acc: 0.8461    \n",
      "Epoch 73/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.5292 - acc: 0.8536    \n",
      "Epoch 74/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.5237 - acc: 0.8534    \n",
      "Epoch 75/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.5126 - acc: 0.8549    \n",
      "Epoch 76/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.4963 - acc: 0.8628    \n",
      "Epoch 77/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.4875 - acc: 0.8616    \n",
      "Epoch 78/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.4874 - acc: 0.8604    \n",
      "Epoch 79/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.4622 - acc: 0.8755    \n",
      "Epoch 80/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.4559 - acc: 0.8738    \n",
      "Epoch 81/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.4362 - acc: 0.8825    \n",
      "Epoch 82/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.4296 - acc: 0.8876    \n",
      "Epoch 83/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.4252 - acc: 0.8829    \n",
      "Epoch 84/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.4119 - acc: 0.8891    \n",
      "Epoch 85/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3980 - acc: 0.8922    \n",
      "Epoch 86/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3926 - acc: 0.8931    \n",
      "Epoch 87/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3854 - acc: 0.8968    \n",
      "Epoch 88/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3716 - acc: 0.9012    \n",
      "Epoch 89/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3664 - acc: 0.8979    \n",
      "Epoch 90/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3560 - acc: 0.9071    \n",
      "Epoch 91/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3433 - acc: 0.9097    \n",
      "Epoch 92/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3445 - acc: 0.9068    \n",
      "Epoch 93/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3263 - acc: 0.9144    \n",
      "Epoch 94/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3288 - acc: 0.9127    \n",
      "Epoch 95/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3291 - acc: 0.9122    \n",
      "Epoch 96/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3113 - acc: 0.9210    \n",
      "Epoch 97/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3039 - acc: 0.9207    \n",
      "Epoch 98/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.3012 - acc: 0.9232    \n",
      "Epoch 99/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2912 - acc: 0.9268    \n",
      "Epoch 100/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2943 - acc: 0.9243    \n",
      "Epoch 101/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2796 - acc: 0.9278    \n",
      "Epoch 102/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2701 - acc: 0.9313    \n",
      "Epoch 103/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2709 - acc: 0.9319    \n",
      "Epoch 104/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2569 - acc: 0.9392    \n",
      "Epoch 105/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2553 - acc: 0.9372    \n",
      "Epoch 106/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2582 - acc: 0.9345    \n",
      "Epoch 107/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2465 - acc: 0.9386    \n",
      "Epoch 108/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2462 - acc: 0.9389    \n",
      "Epoch 109/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2386 - acc: 0.9395    \n",
      "Epoch 110/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2322 - acc: 0.9416    \n",
      "Epoch 111/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2399 - acc: 0.9392    \n",
      "Epoch 112/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2260 - acc: 0.9443    \n",
      "Epoch 113/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2147 - acc: 0.9486    \n",
      "Epoch 114/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2205 - acc: 0.9474    \n",
      "Epoch 115/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2164 - acc: 0.9474    \n",
      "Epoch 116/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2116 - acc: 0.9471    \n",
      "Epoch 117/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2038 - acc: 0.9511    \n",
      "Epoch 118/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.2093 - acc: 0.9478    \n",
      "Epoch 119/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1948 - acc: 0.9534    \n",
      "Epoch 120/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1942 - acc: 0.9534    \n",
      "Epoch 121/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1892 - acc: 0.9553    \n",
      "Epoch 122/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1891 - acc: 0.9552    \n",
      "Epoch 123/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1894 - acc: 0.9554    \n",
      "Epoch 124/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1792 - acc: 0.9562    \n",
      "Epoch 125/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1796 - acc: 0.9590    \n",
      "Epoch 126/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1743 - acc: 0.9606    \n",
      "Epoch 127/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1723 - acc: 0.9588    \n",
      "Epoch 128/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1667 - acc: 0.9594    \n",
      "Epoch 129/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1644 - acc: 0.9636    \n",
      "Epoch 130/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1684 - acc: 0.9602    \n",
      "Epoch 131/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1672 - acc: 0.9595    \n",
      "Epoch 132/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1538 - acc: 0.9665    \n",
      "Epoch 133/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1506 - acc: 0.9669    \n",
      "Epoch 134/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1545 - acc: 0.9640    \n",
      "Epoch 135/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1448 - acc: 0.9655    \n",
      "Epoch 136/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1499 - acc: 0.9647    \n",
      "Epoch 137/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1453 - acc: 0.9672    \n",
      "Epoch 138/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1442 - acc: 0.9681    \n",
      "Epoch 139/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1457 - acc: 0.9666    \n",
      "Epoch 140/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1388 - acc: 0.9677    \n",
      "Epoch 141/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1336 - acc: 0.9717    \n",
      "Epoch 142/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1375 - acc: 0.9686    \n",
      "Epoch 143/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1298 - acc: 0.9725    \n",
      "Epoch 144/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1270 - acc: 0.9719    \n",
      "Epoch 145/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1265 - acc: 0.9714    \n",
      "Epoch 146/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1267 - acc: 0.9720    \n",
      "Epoch 147/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1246 - acc: 0.9737    \n",
      "Epoch 148/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1235 - acc: 0.9725    \n",
      "Epoch 149/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1192 - acc: 0.9733    \n",
      "Epoch 150/150\n",
      "10000/10000 [==============================] - 25s - loss: 0.1209 - acc: 0.9748    \n",
      "CNN acc: 73.45%\n",
      "deep features extracted (4096,)\n",
      "\n",
      "Training SVM..\n",
      "SVM acc: : 65.05%\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "           AisKacang       0.78      0.65      0.71       100\n",
      "           AngKuKueh       0.46      0.57      0.51       100\n",
      "           ApamBalik       0.68      0.69      0.68       100\n",
      "           Asamlaksa       0.57      0.57      0.57       100\n",
      "              Bahulu       0.81      0.97      0.88       100\n",
      "           Bakkukteh       0.73      0.80      0.77       100\n",
      "      BananaLeafRice       0.55      0.61      0.58       100\n",
      "             Bazhang       0.67      0.66      0.67       100\n",
      "         BeefRendang       0.91      0.91      0.91       100\n",
      "           BingkaUbi       0.79      0.88      0.83       100\n",
      "         Buburchacha       0.69      0.66      0.68       100\n",
      "          Buburpedas       0.75      0.69      0.72       100\n",
      "              Capati       0.61      0.58      0.59       100\n",
      "              Cendol       0.58      0.59      0.58       100\n",
      "         ChaiTowKuay       0.85      0.86      0.86       100\n",
      "        CharKuehTiao       0.81      0.68      0.74       100\n",
      "             CharSiu       0.77      0.88      0.82       100\n",
      "       CheeCheongFun       0.78      0.76      0.77       100\n",
      "           ChiliCrab       0.55      0.48      0.51       100\n",
      "           Chweekueh       0.81      0.68      0.74       100\n",
      "         ClayPotRice       0.55      0.50      0.52       100\n",
      "          CucurUdang       0.66      0.54      0.59       100\n",
      "          CurryLaksa       0.47      0.48      0.48       100\n",
      "           CurryPuff       0.54      0.58      0.56       100\n",
      "               Dodol       0.63      0.58      0.60       100\n",
      "              Durian       0.60      0.61      0.60       100\n",
      "         DurianCrepe       0.55      0.65      0.60       100\n",
      "       FishHeadCurry       0.55      0.54      0.54       100\n",
      "               Guava       0.59      0.59      0.59       100\n",
      "HainaneseChickenRice       0.60      0.58      0.59       100\n",
      "          HokkienMee       0.62      0.56      0.59       100\n",
      "            Huatkuih       0.77      0.87      0.82       100\n",
      "           IkanBakar       0.72      0.80      0.76       100\n",
      "            Kangkung       0.66      0.64      0.65       100\n",
      "           KayaToast       0.52      0.59      0.55       100\n",
      "            Keklapis       0.55      0.52      0.54       100\n",
      "             Ketupat       0.68      0.69      0.68       100\n",
      "           KuihDadar       0.47      0.47      0.47       100\n",
      "           KuihLapis       0.78      0.89      0.83       100\n",
      "        KuihSeriMuka       0.57      0.50      0.53       100\n",
      "             Langsat       0.59      0.70      0.64       100\n",
      "               Lekor       0.58      0.59      0.58       100\n",
      "              Lemang       0.73      0.75      0.74       100\n",
      "         LepatPisang       0.52      0.63      0.57       100\n",
      "              LorMee       0.68      0.76      0.72       100\n",
      "        Maggi goreng       0.86      0.84      0.85       100\n",
      "          Mangosteen       0.57      0.54      0.56       100\n",
      "           MeeGoreng       0.58      0.63      0.61       100\n",
      "         MeeHoonKueh       0.73      0.69      0.71       100\n",
      "         MeeHoonSoup       0.55      0.56      0.56       100\n",
      "             MeeJawa       0.72      0.83      0.77       100\n",
      "            MeeRebus       0.71      0.62      0.66       100\n",
      "            MeeRojak       0.61      0.56      0.58       100\n",
      "             MeeSiam       0.63      0.65      0.64       100\n",
      "            Murtabak       0.67      0.63      0.65       100\n",
      "             Murukku       0.68      0.69      0.68       100\n",
      "   NasiGorengKampung       0.69      0.70      0.69       100\n",
      "           NasiImpit       0.63      0.62      0.62       100\n",
      "          Nasikandar       0.72      0.78      0.75       100\n",
      "           Nasilemak       0.49      0.38      0.43       100\n",
      "         Nasipattaya       0.75      0.74      0.74       100\n",
      "          Ondehondeh       0.65      0.49      0.56       100\n",
      "            Otakotak       0.73      0.80      0.76       100\n",
      "      OysterOmelette       0.64      0.65      0.65       100\n",
      "              PanMee       0.52      0.49      0.50       100\n",
      "       PineappleTart       0.49      0.47      0.48       100\n",
      "        PisangGoreng       0.64      0.69      0.66       100\n",
      "              Popiah       0.70      0.69      0.70       100\n",
      "            PrawnMee       0.48      0.61      0.54       100\n",
      "         Prawnsambal       0.65      0.72      0.69       100\n",
      "                Puri       0.70      0.69      0.69       100\n",
      "           PutuMayam       0.56      0.57      0.57       100\n",
      "          PutuPiring       0.49      0.39      0.43       100\n",
      "            Rambutan       0.48      0.40      0.43       100\n",
      "               Rojak       0.90      0.95      0.93       100\n",
      "           RotiCanai       0.60      0.52      0.56       100\n",
      "            RotiJala       0.73      0.68      0.70       100\n",
      "            RotiJohn       0.76      0.81      0.78       100\n",
      "            RotiNaan       0.62      0.62      0.62       100\n",
      "          RotiTissue       0.54      0.44      0.48       100\n",
      "         SambalPetai       0.71      0.67      0.69       100\n",
      "         SambalUdang       0.60      0.76      0.67       100\n",
      "               Satay       0.65      0.73      0.69       100\n",
      "          Sataycelup       0.73      0.69      0.71       100\n",
      "            SeriMuka       0.94      0.90      0.92       100\n",
      "            SotoAyam       0.72      0.75      0.74       100\n",
      "     TandooriChicken       0.59      0.58      0.59       100\n",
      "            TangYuan       0.73      0.63      0.68       100\n",
      "           TauFooFah       0.83      0.85      0.84       100\n",
      "         TauhuSumbat       0.53      0.46      0.49       100\n",
      "              Thosai       0.59      0.63      0.61       100\n",
      "          TomYumSoup       0.87      0.81      0.84       100\n",
      "               Wajik       0.51      0.49      0.50       100\n",
      "           WanTanMee       0.60      0.63      0.61       100\n",
      "             WaTanHo       0.56      0.61      0.59       100\n",
      "              Wonton       0.54      0.50      0.52       100\n",
      "             YamCake       0.48      0.48      0.48       100\n",
      "           YongTauFu       0.76      0.81      0.79       100\n",
      "             Youtiao       0.63      0.46      0.53       100\n",
      "             Yusheng       0.66      0.69      0.68       100\n",
      "\n",
      "         avg / total       0.65      0.65      0.65     10000\n",
      "\n",
      "[[65  0  0 ...,  0  0  2]\n",
      " [ 0 57  0 ...,  0  1  0]\n",
      " [ 0  0 69 ...,  0  0  0]\n",
      " ..., \n",
      " [ 0  0  0 ..., 81  0  0]\n",
      " [ 0  2  0 ...,  0 46  0]\n",
      " [ 2  0  0 ...,  0  0 69]]\n",
      "Average acc : 65.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode='max')\n",
      "/home/machine/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode='max')\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Flatten, Dense, Dropout, Reshape, Permute, Activation, \\\n",
    "    Input, merge\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from convnetskeras.customlayers import convolution2Dgroup, crosschannelnormalization, \\\n",
    "    splittensor, Softmax4D\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.utils import np_utils\n",
    "from sklearn import svm\n",
    "\n",
    "import sys, glob, argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "\n",
    "import hickle as hkl\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# path to the model weights file.\n",
    "weights_path = '../dataset/alexnet_weights.h5'\n",
    "nb_class = 100\n",
    "nb_epoch = 150\n",
    "fold_count = 1\n",
    "\n",
    "def get_layer_weights(weights_file=None, layer_name=None):\n",
    "    if not weights_file or not layer_name:\n",
    "        return None\n",
    "    else:\n",
    "        g = weights_file[layer_name]\n",
    "        weights = [g[p] for p in g]\n",
    "        print 'Weights for \"{}\" are loaded'.format(layer_name)\n",
    "    return weights\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.jet):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(100)\n",
    "    plt.xticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "         'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    "         'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    "         'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    "         'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    "         'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    "         'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    "         'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    "         'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    "         'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    "         'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    "         'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    "         'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    "         'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    "         'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    "         'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    "         'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    "         'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "         'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    "         'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    "         'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    "         'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    "         'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    "         'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    "         'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    "         'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    "         'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    "         'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    "         'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    "         'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    "         'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    "         'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    "         'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    "         'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    "         'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def visualize_loss(hist):\n",
    "    train_loss=hist.history['loss']\n",
    "    val_loss=hist.history['val_loss']\n",
    "    train_acc=hist.history['acc']\n",
    "    val_acc=hist.history['val_acc']\n",
    "    xc=range(nb_epoch)\n",
    "\n",
    "    plt.figure(1,figsize=(7,5))\n",
    "    plt.plot(xc,train_loss)\n",
    "    plt.plot(xc,val_loss)\n",
    "    plt.xlabel('num of Epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('train_loss vs val_loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['train','val'])\n",
    "    #print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "    plt.style.use(['classic'])\n",
    "\n",
    "    plt.figure(2,figsize=(7,5))\n",
    "    plt.plot(xc,train_acc)\n",
    "    plt.plot(xc,val_acc)\n",
    "    plt.xlabel('num of Epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('train_acc vs val_acc')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['train','val'],loc=4)\n",
    "    #print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "    plt.style.use(['classic'])\n",
    "    plt.show()\n",
    "\n",
    "def load_data():\n",
    "    # load your data using this function\n",
    "    d = hkl.load('../dataset/myfood100-227.hkl')\n",
    "    data = d['trainFeatures']\n",
    "    labels = d['trainLabels']\n",
    "    lz = d['labels']\n",
    "    data = data.reshape(data.shape[0], 3, 227, 227)\n",
    "    #data = data.transpose(0, 2, 3, 1)\n",
    "\n",
    "    return data,labels,lz\n",
    "\n",
    "def get_top_model_for_alexnet(nb_class=None, shape=None, W_regularizer=False, weights_file_path=None, input=None, output=None):\n",
    "    if not output:\n",
    "        inputs = Input(shape=shape)\n",
    "    \n",
    "    \n",
    "    dense_1 = Flatten(name=\"flatten\")(inputs)\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1')(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2')(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(nb_class,name='dense_3')(dense_3)\n",
    "    predictions = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "    model = Model(input=input or inputs, output=predictions)\n",
    "\n",
    "    return model\n",
    "\n",
    "# This function load top model pretrained weights, will be merge with alexnet bottom model weight\n",
    "def get_top_model_for_alexnet2(nb_class=None, shape=None, W_regularizer=False, weights_file_path=None, input=None, output=None):\n",
    "    if not output:\n",
    "        inputs = Input(shape=shape)\n",
    "        dense_1 = Flatten(name='flatten')(inputs)\n",
    "    else:\n",
    "        dense_1 = Flatten(name='flatten', input_shape=shape)(output)\n",
    "\n",
    "    if weights_file_path:\n",
    "        weights_file = h5.File(weights_file_path)\n",
    "\n",
    "    weights_1 = get_layer_weights(weights_file, 'dense_1')\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1',weights=weights_1)(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "\n",
    "    weights_2 = get_layer_weights(weights_file, 'dense_2')\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2',weights=weights_2)(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "\n",
    "    weights_3 = get_layer_weights(weights_file, 'dense_3')\n",
    "    dense_3 = Dense(nb_class,name='dense_3',weights=weights_3)(dense_3)\n",
    "    predictions = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "    \n",
    "    model = Model(input=input or inputs, output=predictions)\n",
    "\n",
    "\n",
    "    if weights_file:\n",
    "        weights_file.close()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Model for extracting bottleneck \n",
    "def load_model(nb_class, weights_path=None):\n",
    "\n",
    "    inputs = Input(shape=(3,227,227))\n",
    "\n",
    "    conv_1 = Convolution2D(96, 11, 11,subsample=(4,4),activation='relu',\n",
    "                           name='conv_1')(inputs)\n",
    "\n",
    "    conv_2 = MaxPooling2D((3, 3), strides=(2,2))(conv_1)\n",
    "    conv_2 = crosschannelnormalization(name=\"convpool_1\")(conv_2)\n",
    "    conv_2 = ZeroPadding2D((2,2))(conv_2)\n",
    "    conv_2 = merge([\n",
    "        Convolution2D(128,5,5,activation=\"relu\",name='conv_2_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_2)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_2\")\n",
    "\n",
    "    conv_3 = MaxPooling2D((3, 3), strides=(2, 2))(conv_2)\n",
    "    conv_3 = crosschannelnormalization()(conv_3)\n",
    "    conv_3 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_3 = Convolution2D(384,3,3,activation='relu',name='conv_3')(conv_3)\n",
    "\n",
    "    conv_4 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_4 = merge([\n",
    "        Convolution2D(192,3,3,activation=\"relu\",name='conv_4_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_4)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_4\")\n",
    "\n",
    "    conv_5 = ZeroPadding2D((1,1))(conv_4)\n",
    "    conv_5 = merge([\n",
    "        Convolution2D(128,3,3,activation=\"relu\",name='conv_5_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_5)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_5\")\n",
    "\n",
    "    conv_5 = MaxPooling2D((3, 3), strides=(2,2),name=\"convpool_5\")(conv_5)\n",
    "\n",
    "    dense_1 = Flatten(name=\"flatten\")(conv_5)\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1')(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2')(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(nb_class,name='dense_3')(dense_3)\n",
    "    prediction = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "\n",
    "\n",
    "    base_model = Model(input=inputs, output=prediction)\n",
    "\n",
    "\n",
    "    base_model.load_weights(weights_path)\n",
    "\n",
    "\n",
    "    #model = Model(input=inputs, output=conv_5)\n",
    "\n",
    "    base_model = Model(input=inputs, output=conv_5)\n",
    "\n",
    "    for layer in base_model.layers[:17]:\n",
    "        print layer\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = get_top_model_for_alexnet2(\n",
    "        shape=base_model.output_shape[1:],\n",
    "        nb_class=nb_class,\n",
    "        weights_file_path=\"model/alex_topmodel\" + str(fold_count) + \".h5\",\n",
    "        input= base_model.input,\n",
    "        output= base_model.output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Model for merging bottom alexnet model weights and finetuned top model weights \n",
    "def load_svm_model(nb_class, weights_path=None):\n",
    "\n",
    "    inputs = Input(shape=(3,227,227))\n",
    "\n",
    "    conv_1 = Convolution2D(96, 11, 11,subsample=(4,4),activation='relu',\n",
    "                           name='conv_1')(inputs)\n",
    "\n",
    "    conv_2 = MaxPooling2D((3, 3), strides=(2,2))(conv_1)\n",
    "    conv_2 = crosschannelnormalization(name=\"convpool_1\")(conv_2)\n",
    "    conv_2 = ZeroPadding2D((2,2))(conv_2)\n",
    "    conv_2 = merge([\n",
    "        Convolution2D(128,5,5,activation=\"relu\",name='conv_2_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_2)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_2\")\n",
    "\n",
    "    conv_3 = MaxPooling2D((3, 3), strides=(2, 2))(conv_2)\n",
    "    conv_3 = crosschannelnormalization()(conv_3)\n",
    "    conv_3 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_3 = Convolution2D(384,3,3,activation='relu',name='conv_3')(conv_3)\n",
    "\n",
    "    conv_4 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_4 = merge([\n",
    "        Convolution2D(192,3,3,activation=\"relu\",name='conv_4_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_4)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_4\")\n",
    "\n",
    "    conv_5 = ZeroPadding2D((1,1))(conv_4)\n",
    "    conv_5 = merge([\n",
    "        Convolution2D(128,3,3,activation=\"relu\",name='conv_5_'+str(i+1))(\n",
    "            splittensor(ratio_split=2,id_split=i)(conv_5)\n",
    "        ) for i in range(2)], mode='concat',concat_axis=1,name=\"conv_5\")\n",
    "\n",
    "    conv_5 = MaxPooling2D((3, 3), strides=(2,2),name=\"convpool_5\")(conv_5)\n",
    "\n",
    "    dense_1 = Flatten(name=\"flatten\")(conv_5)\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1')(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2')(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(nb_class,name='dense_3')(dense_3)\n",
    "    prediction = Activation(\"softmax\",name=\"softmax\")(dense_3)\n",
    "\n",
    "\n",
    "    base_model = Model(input=inputs, output=prediction)\n",
    "\n",
    "    if weights_path:\n",
    "        base_model.load_weights(weights_path)\n",
    "\n",
    "\n",
    "    #model = Model(input=inputs, output=conv_5)\n",
    "\n",
    "    base_model = Model(input=inputs, output=dense_2)\n",
    "\n",
    "    '''\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = get_top_model_for_svm(\n",
    "        shape=base_model.output_shape[1:],\n",
    "        nb_class=nb_class,\n",
    "        weights_file_path=\"model/alex_topmodel\" + str(fold_count) + \".h5\",\n",
    "        input= base_model.input,\n",
    "        output= base_model.output)\n",
    "    '''\n",
    "\n",
    "    return base_model\n",
    "\n",
    "\n",
    "# Save CNN bottleneck for finetune top model \n",
    "def save_bottleneck_features(X_train, X_test, y_train, y_test):\n",
    "    model = load_model(nb_class=nb_class, weights_path=weights_path)\n",
    "\n",
    "    bottleneck_features_train = model.predict(X_train)\n",
    "    np.save(open('alex_bottleneck_features_train'+ str(fold_count) +'.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "\n",
    "    bottleneck_features_validation = model.predict(X_test)\n",
    "    np.save(open('alex_bottleneck_features_validation'+ str(fold_count) + '.npy', 'wb'), bottleneck_features_validation)\n",
    "    print \"deep features extracted\", bottleneck_features_train.shape[1:]\n",
    "\n",
    "# Save finetuned CNN for svm classification\n",
    "def save_bottleneck_svmfeatures(X_train, X_test, y_train, y_test, pretrained_weights):\n",
    "    model = load_svm_model(nb_class=nb_class, weights_path=pretrained_weights)\n",
    "\n",
    "    bottleneck_features_train = model.predict(X_train)\n",
    "    np.save(open('alex_doblefinetune_svmfeatures_train'+ str(fold_count) +'.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "\n",
    "    bottleneck_features_validation = model.predict(X_test)\n",
    "    np.save(open('alex_doblefinetune_svmfeatures_validation'+ str(fold_count) + '.npy', 'wb'), bottleneck_features_validation)\n",
    "    print \"deep features extracted\", bottleneck_features_train.shape[1:]\n",
    "\n",
    "# Train top model and save weithgs \n",
    "def train_top_model(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    model = load_model(nb_class=nb_class, weights_path=weights_path)\n",
    "    \n",
    "    print \"\\nTraining CNN..\"\n",
    "    y_train = np_utils.to_categorical(y_train, nb_class)\n",
    "    y_test = np_utils.to_categorical(y_test, nb_class)\n",
    "\n",
    "    shape=X_train.shape[1:]\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    hist = model.fit(X_train, y_train,\n",
    "              nb_epoch=nb_epoch, batch_size=32,verbose=1)\n",
    "    \n",
    "    #visualize_loss(hist) # Removing this because we not training CNN, just classification \n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    model.save_weights(\"model/alex_dobletune\" + str(fold_count) + \".h5\")\n",
    "    #model.save_weights(\"model/alex_topmodel\" + str(fold_count) + \".h5\")\n",
    "    print(\"CNN %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "    return scores[1]\n",
    "\n",
    "# SVM classification \n",
    "def train_svm(y_train, y_test):\n",
    "    svm_train = np.load(open('alex_doblefinetune_svmfeatures_train'+ str(fold_count) +'.npy' , 'rb'))\n",
    "    svm_test = np.load(open('alex_doblefinetune_svmfeatures_validation'+ str(fold_count) + '.npy', 'rb'))\n",
    "\n",
    "\n",
    "    print \"\\nTraining SVM..\"\n",
    "    clf = svm.SVC(kernel='linear', gamma=0.7, C=1.0)\n",
    "\n",
    "    clf.fit(svm_train, y_train.ravel())\n",
    "    #y_pred = clf.predict(test_data)\n",
    "    score = clf.score(svm_test, y_test.ravel())\n",
    "    print(\"SVM %s: %.2f%%\" % (\"acc: \", score*100))\n",
    "    \n",
    "    y_pred = clf.predict(svm_test)\n",
    "    \n",
    "    target_names = ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "     'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    "     'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    "     'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    "     'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    "     'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    "     'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    "     'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    "     'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    "     'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    "     'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    "     'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    "     'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    "     'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    "     'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    "     'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    "     'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    "     'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng']\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(classification_report(y_test, y_pred,target_names=target_names))\n",
    "    print(cm)\n",
    "    \n",
    "    return score\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    n_folds = 2\n",
    "    total_scores = 0\n",
    "    \n",
    "    print \"Loading data..\"\n",
    "    data, labels, lz = load_data()\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    lz = np.array(lz)\n",
    "    print lz.shape\n",
    "    print \"Data loaded !\"\n",
    "    \n",
    "    skf = StratifiedKFold(y=lz, n_folds=n_folds, shuffle=False)\n",
    "    \n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print \"Test train Shape: \"\n",
    "        print data[train].shape\n",
    "        print data[test].shape\n",
    "        print (\"Running Fold %d / %d\" % (i+1, n_folds))\n",
    "        \n",
    "        #save_bottleneck_features(data[train], data[test],labels[train], labels[test])\n",
    "        scores = train_top_model(data[train], data[test],labels[train], labels[test])\n",
    "        \n",
    "        save_bottleneck_svmfeatures(data[train], data[test],labels[train], labels[test],\"model/alex_dobletune\" + str(fold_count) + \".h5\")\n",
    "        svm_scores = train_svm(labels[train], labels[test])\n",
    "        \n",
    "        total_scores = total_scores + svm_scores\n",
    "        fold_count = fold_count + 1\n",
    "    print(\"Average acc : %.2f%%\" % (total_scores/n_folds*100))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
