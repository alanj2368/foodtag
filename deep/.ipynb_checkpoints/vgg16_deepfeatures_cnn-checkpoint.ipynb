{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "Data loaded !\n",
      "(1700, 3, 224, 224)\n",
      "(300, 3, 224, 224)\n",
      "Test train splitted !\n",
      "(1700, 512, 7, 7)\n",
      "(300, 512, 7, 7)\n",
      "Training CNN..\n",
      "Epoch 1/10\n",
      "1700/1700 [==============================] - 9s - loss: 42.2689 - acc: 0.0953     \n",
      "Epoch 2/10\n",
      "1700/1700 [==============================] - 9s - loss: 13.5734 - acc: 0.1053     \n",
      "Epoch 3/10\n",
      "1700/1700 [==============================] - 9s - loss: 2.0164 - acc: 0.4353     \n",
      "Epoch 4/10\n",
      "1700/1700 [==============================] - 9s - loss: 1.3126 - acc: 0.6465     \n",
      "Epoch 5/10\n",
      "1700/1700 [==============================] - 9s - loss: 1.0441 - acc: 0.7512     \n",
      "Epoch 6/10\n",
      "1700/1700 [==============================] - 9s - loss: 0.8862 - acc: 0.7976     \n",
      "Epoch 7/10\n",
      "1700/1700 [==============================] - 9s - loss: 0.7464 - acc: 0.8482     \n",
      "Epoch 8/10\n",
      "1700/1700 [==============================] - 9s - loss: 0.6281 - acc: 0.8853     \n",
      "Epoch 9/10\n",
      "1700/1700 [==============================] - 9s - loss: 0.5727 - acc: 0.9053     \n",
      "Epoch 10/10\n",
      "1700/1700 [==============================] - 9s - loss: 0.5270 - acc: 0.9259     \n",
      "acc: 85.67%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import column_or_1d\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import svm\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "import util\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# path to the model weights file.\n",
    "weights_path = '../dataset/vgg16_weights.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "f_model = './model'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "nb_classes = 10\n",
    "\n",
    "nb_train_samples = 1700\n",
    "nb_validation_samples = 300\n",
    "nb_epoch = 10\n",
    "\n",
    "\n",
    "def get_layer_weights(weights_file=None, layer_name=None):\n",
    "    if not weights_file or not layer_name:\n",
    "        return None\n",
    "    else:\n",
    "        g = weights_file[layer_name]\n",
    "        weights = [g[p] for p in g]\n",
    "        print 'Weights for \"{}\" are loaded'.format(layer_name)\n",
    "        return weights\n",
    "\n",
    "def load_data():\n",
    "    # load your data using this function\n",
    "    f = open(\"../dataset/myfood10-224.pkl\", 'rb')\n",
    "    d = pickle.load(f)\n",
    "    data = d['trainFeatures']\n",
    "    labels = d['trainLabels']\n",
    "    lz = d['labels']\n",
    "    data = data.reshape(data.shape[0], 3, 224, 224)\n",
    "    #data = data.transpose(0, 2, 3, 1)\n",
    "\n",
    "    return data,labels,lz\n",
    "\n",
    "def save_bottlebeck_features(X_train, X_test, y_train, y_test):\n",
    "    weights_path='../dataset/vgg16_weights.h5'\n",
    "    # build the VGG16 network\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "    assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n",
    "    f = h5py.File(weights_path)\n",
    "    for k in range(f.attrs['nb_layers']):\n",
    "        if k >= len(model.layers):\n",
    "            # we don't look at the last (fully-connected) layers in the savefile\n",
    "            break\n",
    "        g = f['layer_{}'.format(k)]\n",
    "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "        model.layers[k].set_weights(weights)\n",
    "    f.close()\n",
    "    print('Model loaded.')\n",
    "\n",
    "    bottleneck_features_train = model.predict(X_train, batch_size=32)\n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "\n",
    "    bottleneck_features_validation = model.predict(X_test, batch_size=32)\n",
    "    np.save(open('bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.jet):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(10)\n",
    "    plt.xticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "    'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "    'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi'])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "'''\n",
    "def train_top_model(y_train, y_test):\n",
    "    train_data = np.load(open('bottleneck_features_train.npy' , 'rb'))\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))\n",
    "\n",
    "\n",
    "    print train_data.shape\n",
    "    print validation_data.shape\n",
    "    print \"Training CNN..\"\n",
    "\n",
    "    input = None\n",
    "    shape=train_data.shape[1:]\n",
    "    output = None\n",
    "    W_regularizer=True\n",
    "    weights_file_path = None\n",
    "    weights_file = None\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(name='flatten', input_shape=shape))\n",
    "    W_regularizer = l2(1e-2)\n",
    "    model.add(Dense(4096, activation='relu',W_regularizer=W_regularizer))\n",
    "    model.add(Dropout(0.6))\n",
    "    W_regularizer = l2(1e-2)\n",
    "    model.add(Dense(4096, activation='relu',W_regularizer=W_regularizer))\n",
    "    model.add(Dense(nb_classes,activation='softmax'))\n",
    "\n",
    "    rms = RMSprop(lr=5e-4, rho=0.9, epsilon=1e-08, decay=0.01)\n",
    "    model.compile(optimizer=rms, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, y_train,\n",
    "              nb_epoch=nb_epoch, batch_size=32,\n",
    "              validation_data=(validation_data, y_test))\n",
    "\n",
    "    Y_pred = model.predict(validation_data)\n",
    "    print(Y_pred)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    print(y_pred)\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    print \"Loading data..\"\n",
    "    data, labels, lz = load_data()\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    lz = np.array(lz)\n",
    "    print \"Data loaded !\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.15, random_state=seed)\n",
    "    print X_train.shape\n",
    "    print X_test.shape\n",
    "    print \"Test train splitted !\"\n",
    "\n",
    "    #save_bottlebeck_features(X_train, X_test, y_train, y_test)\n",
    "    #train_top_model(y_train, y_test)\n",
    "    train_data = np.load(open('bottleneck_features_train.npy' , 'rb'))\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))\n",
    "\n",
    "\n",
    "    print train_data.shape\n",
    "    print validation_data.shape\n",
    "    print \"Training CNN..\"\n",
    "\n",
    "    input = None\n",
    "    shape=train_data.shape[1:]\n",
    "    output = None\n",
    "    W_regularizer=True\n",
    "    weights_file_path = None\n",
    "    weights_file = None\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(name='flatten', input_shape=shape))\n",
    "    W_regularizer = l2(1e-2)\n",
    "    model.add(Dense(4096, activation='relu',W_regularizer=W_regularizer))\n",
    "    model.add(Dropout(0.6))\n",
    "    W_regularizer = l2(1e-2)\n",
    "    model.add(Dense(4096, activation='relu',W_regularizer=W_regularizer))\n",
    "    model.add(Dense(nb_classes,activation='softmax'))\n",
    "\n",
    "    rms = RMSprop(lr=5e-4, rho=0.9, epsilon=1e-08, decay=0.01)\n",
    "    model.compile(optimizer=rms, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, y_train, nb_epoch=nb_epoch, batch_size=32)\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)\n",
    "    json_string = model.to_json()\n",
    "    open(os.path.join(f_model,'test_model.json'), 'w').write(json_string)\n",
    "\n",
    "    scores = model.evaluate(validation_data, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'test_model.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-571d0472b7f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_scale_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_model.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bottleneck_fc_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-571d0472b7f2>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_def_fname, model_weight_fname)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_def_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_weight_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m    \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_def_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weight_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'test_model.json'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    " \n",
    "def load_and_scale_imgs():\n",
    "   img_names = ['../../../Pictures/a.jpg'\n",
    "               ]\n",
    " \n",
    "   imgs = [np.transpose(scipy.misc.imresize(scipy.misc.imread(img_name), (224, 224)),\n",
    "                        (2, 0, 1)).astype('float32')\n",
    "           for img_name in img_names]\n",
    "\n",
    "\n",
    "   return np.array(imgs) / 255\n",
    "\n",
    "def load_model(model_def_fname, model_weight_fname):\n",
    "   model = model_from_json(open(model_def_fname).read())\n",
    "   model.load_weights(model_weight_fname)\n",
    "   return model\n",
    "\n",
    "imgs = load_and_scale_imgs()\n",
    "model = load_model('model/test_model.json', 'bottleneck_fc_model.h5')\n",
    "model.summary()\n",
    "\n",
    "# train\n",
    "optim = RMSprop()\n",
    "#optim = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "predictions = model.predict_classes(imgs)\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
